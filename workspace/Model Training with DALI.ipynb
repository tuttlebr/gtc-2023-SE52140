{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d6d0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "from nvidia.dali.plugin.pytorch import DALIClassificationIterator, LastBatchPolicy\n",
    "from nvidia.dali.pipeline import pipeline_def\n",
    "import nvidia.dali.types as types\n",
    "import nvidia.dali.fn as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a954534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global DDP, amp, optimizers, parallel\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "from apex import amp, optimizers, parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9ff42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_python_float(t):\n",
    "    if hasattr(t, \"item\"):\n",
    "        return t.item()\n",
    "    else:\n",
    "        return t[0]\n",
    "\n",
    "\n",
    "@pipeline_def\n",
    "def create_dali_pipeline(\n",
    "    data_dir, crop, size, shard_id, num_shards, dali_cpu=False, is_training=True\n",
    "):\n",
    "    images, labels = fn.readers.caffe(\n",
    "        path=data_dir,\n",
    "        shard_id=shard_id,\n",
    "        num_shards=num_shards,\n",
    "        random_shuffle=is_training,\n",
    "        pad_last_batch=True,\n",
    "        name=\"Reader\",\n",
    "    )\n",
    "\n",
    "    dali_device = \"cpu\" if dali_cpu else \"gpu\"\n",
    "    decoder_device = \"cpu\" if dali_cpu else \"mixed\"\n",
    "    # ask nvJPEG to preallocate memory for the biggest sample in ImageNet for CPU and GPU to avoid reallocations in runtime\n",
    "    device_memory_padding = 211025920 if decoder_device == \"mixed\" else 0\n",
    "    host_memory_padding = 140544512 if decoder_device == \"mixed\" else 0\n",
    "    # ask HW NVJPEG to allocate memory ahead for the biggest image in the data set to avoid reallocations in runtime\n",
    "    preallocate_width_hint = 5980 if decoder_device == \"mixed\" else 0\n",
    "    preallocate_height_hint = 6430 if decoder_device == \"mixed\" else 0\n",
    "    if is_training:\n",
    "        images = fn.decoders.image_random_crop(\n",
    "            images,\n",
    "            device=decoder_device,\n",
    "            output_type=types.RGB,\n",
    "            device_memory_padding=device_memory_padding,\n",
    "            host_memory_padding=host_memory_padding,\n",
    "            preallocate_width_hint=preallocate_width_hint,\n",
    "            preallocate_height_hint=preallocate_height_hint,\n",
    "            random_aspect_ratio=[0.8, 1.25],\n",
    "            random_area=[0.1, 1.0],\n",
    "            num_attempts=100,\n",
    "        )\n",
    "        images = fn.resize(\n",
    "            images,\n",
    "            device=dali_device,\n",
    "            resize_x=crop,\n",
    "            resize_y=crop,\n",
    "            interp_type=types.INTERP_TRIANGULAR,\n",
    "        )\n",
    "        mirror = fn.random.coin_flip(probability=0.5)\n",
    "    else:\n",
    "        images = fn.decoders.image(images, device=decoder_device, output_type=types.RGB)\n",
    "        images = fn.resize(\n",
    "            images,\n",
    "            device=dali_device,\n",
    "            size=size,\n",
    "            mode=\"not_smaller\",\n",
    "            interp_type=types.INTERP_TRIANGULAR,\n",
    "        )\n",
    "        mirror = False\n",
    "\n",
    "    images = fn.crop_mirror_normalize(\n",
    "        images.gpu(),\n",
    "        dtype=types.FLOAT,\n",
    "        output_layout=\"CHW\",\n",
    "        crop=(crop, crop),\n",
    "        mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "        std=[0.229 * 255, 0.224 * 255, 0.225 * 255],\n",
    "        mirror=mirror,\n",
    "    )\n",
    "    labels = labels.gpu()\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fff2d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        input = data[0][\"data\"]\n",
    "        target = data[0][\"label\"].squeeze(-1).long()\n",
    "        train_loader_len = int(math.ceil(train_loader._size / batch_size))\n",
    "\n",
    "        if prof >= 0 and i == prof:\n",
    "            print(\"Profiling begun at iteration {}\".format(i))\n",
    "            torch.cuda.cudart().cudaProfilerStart()\n",
    "\n",
    "        if prof >= 0:\n",
    "            torch.cuda.nvtx.range_push(\"Body of iteration {}\".format(i))\n",
    "\n",
    "        adjust_learning_rate(optimizer, epoch, i, train_loader_len, lr)\n",
    "        if test:\n",
    "            if i > 10:\n",
    "                break\n",
    "\n",
    "        # compute output\n",
    "        if prof >= 0:\n",
    "            torch.cuda.nvtx.range_push(\"forward\")\n",
    "        output = model(input)\n",
    "        if prof >= 0:\n",
    "            torch.cuda.nvtx.range_pop()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if prof >= 0:\n",
    "            torch.cuda.nvtx.range_push(\"backward\")\n",
    "        if opt_level is not None:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        if prof >= 0:\n",
    "            torch.cuda.nvtx.range_pop()\n",
    "\n",
    "        if prof >= 0:\n",
    "            torch.cuda.nvtx.range_push(\"optimizer.step()\")\n",
    "        optimizer.step()\n",
    "        if prof >= 0:\n",
    "            torch.cuda.nvtx.range_pop()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            # Every print_freq iterations, check the loss, accuracy, and speed.\n",
    "            # For best performance, it doesn't make sense to print these metrics every\n",
    "            # iteration, since they incur an allreduce and some host<->device syncs.\n",
    "\n",
    "            # Measure accuracy\n",
    "            prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "\n",
    "            # Average loss and accuracy across processes for logging\n",
    "            if distributed:\n",
    "                reduced_loss = reduce_tensor(loss.data)\n",
    "                prec1 = reduce_tensor(prec1)\n",
    "                prec5 = reduce_tensor(prec5)\n",
    "            else:\n",
    "                reduced_loss = loss.data\n",
    "\n",
    "            # to_python_float incurs a host<->device sync\n",
    "            losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "            top1.update(to_python_float(prec1), input.size(0))\n",
    "            top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            batch_time.update((time.time() - end) / print_freq)\n",
    "            end = time.time()\n",
    "\n",
    "            if local_rank == 0:\n",
    "                print(\n",
    "                    \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                    \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Speed {3:.3f} ({4:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.10f} ({loss.avg:.4f})\\t\"\n",
    "                    \"Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\"\n",
    "                    \"Prec@5 {top5.val:.3f} ({top5.avg:.3f})\".format(\n",
    "                        epoch,\n",
    "                        i,\n",
    "                        train_loader_len,\n",
    "                        world_size * batch_size / batch_time.val,\n",
    "                        world_size * batch_size / batch_time.avg,\n",
    "                        batch_time=batch_time,\n",
    "                        loss=losses,\n",
    "                        top1=top1,\n",
    "                        top5=top5,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Pop range \"Body of iteration {}\".format(i)\n",
    "        if prof >= 0:\n",
    "            torch.cuda.nvtx.range_pop()\n",
    "\n",
    "        if prof >= 0 and i == prof + 10:\n",
    "            print(\"Profiling ended at iteration {}\".format(i))\n",
    "            torch.cuda.cudart().cudaProfilerStop()\n",
    "            quit()\n",
    "\n",
    "    return batch_time.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "        input = data[0][\"data\"]\n",
    "        target = data[0][\"label\"].squeeze(-1).long()\n",
    "        val_loader_len = int(val_loader._size / batch_size)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "\n",
    "        if distributed:\n",
    "            reduced_loss = reduce_tensor(loss.data)\n",
    "            prec1 = reduce_tensor(prec1)\n",
    "            prec5 = reduce_tensor(prec5)\n",
    "        else:\n",
    "            reduced_loss = loss.data\n",
    "\n",
    "        losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "        top1.update(to_python_float(prec1), input.size(0))\n",
    "        top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # TODO:  Change timings to mirror train().\n",
    "        if local_rank == 0 and i % print_freq == 0:\n",
    "            print(\n",
    "                \"Test: [{0}/{1}]\\t\"\n",
    "                \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                \"Speed {2:.3f} ({3:.3f})\\t\"\n",
    "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\"\n",
    "                \"Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\"\n",
    "                \"Prec@5 {top5.val:.3f} ({top5.avg:.3f})\".format(\n",
    "                    i,\n",
    "                    val_loader_len,\n",
    "                    world_size * batch_size / batch_time.val,\n",
    "                    world_size * batch_size / batch_time.avg,\n",
    "                    batch_time=batch_time,\n",
    "                    loss=losses,\n",
    "                    top1=top1,\n",
    "                    top5=top5,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(\" * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}\".format(top1=top1, top5=top5))\n",
    "\n",
    "    return [top1.avg, top5.avg]\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=\"checkpoint.pth.tar\"):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, \"model_best.pth.tar\")\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, step, len_epoch, lr):\n",
    "    factor = epoch // 30\n",
    "\n",
    "    if epoch >= 80:\n",
    "        factor = factor + 1\n",
    "\n",
    "    lr = lr * (0.1**factor)\n",
    "\n",
    "    \"\"\"Warmup\"\"\"\n",
    "    if epoch < 5:\n",
    "        lr = lr * float(1 + step + epoch * len_epoch) / (5.0 * len_epoch)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= world_size\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b178062",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_level = \"O3\"\n",
    "keep_batchnorm_fp32 = True\n",
    "loss_scale = \"dynamic\"\n",
    "batch_size = 256\n",
    "sync_bn = True\n",
    "arch = \"resnet50\"\n",
    "channels_last = False\n",
    "start_epoch = 0\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "prof = -1\n",
    "local_rank = 0\n",
    "workers = 4\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "print_freq = 10\n",
    "resume = \"\"\n",
    "data_root = os.environ[\"DALI_EXTRA_PATH\"]\n",
    "traindir = os.path.join(data_root, \"db\", \"lmdb\")\n",
    "valdir = os.path.join(data_root, \"db\", \"lmdb\")\n",
    "test = False\n",
    "distributed = False\n",
    "dali_cpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cdfe6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "best_prec1 = 0\n",
    "gpu = 1\n",
    "world_size = 2\n",
    "total_batch_size = world_size * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c731f8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> opt_level = O3\n",
      "=> keep_batchnorm_fp32 = True <class 'bool'>\n",
      "=> loss_scale = dynamic <class 'str'>\n",
      "=> CUDNN VERSION: 8700\n",
      "=> creating model 'resnet50'\n",
      "=> using apex synced BN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.parallel.convert_syncbn_model is deprecated and will be removed by the end of February 2023. Use `torch.nn.SyncBatchNorm.convert_sync_batchnorm`.\n",
      "  warnings.warn(msg, DeprecatedFeatureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"=> opt_level = {}\".format(opt_level))\n",
    "print(\n",
    "    \"=> keep_batchnorm_fp32 = {}\".format(keep_batchnorm_fp32), type(keep_batchnorm_fp32)\n",
    ")\n",
    "print(\"=> loss_scale = {}\".format(loss_scale), type(loss_scale))\n",
    "print(\"=> CUDNN VERSION: {}\".format(torch.backends.cudnn.version()))\n",
    "print(\"=> creating model '{}'\".format(arch))\n",
    "model = models.__dict__[arch]()\n",
    "\n",
    "if sync_bn:\n",
    "    print(\"=> using apex synced BN\")\n",
    "    model = parallel.convert_syncbn_model(model)\n",
    "\n",
    "if hasattr(torch, \"channels_last\") and hasattr(torch, \"contiguous_format\"):\n",
    "    if channels_last:\n",
    "        memory_format = torch.channels_last\n",
    "    else:\n",
    "        memory_format = torch.contiguous_format\n",
    "    model = model.cuda().to(memory_format=memory_format)\n",
    "else:\n",
    "    model = model.cuda()\n",
    "\n",
    "lr = lr * float(batch_size * world_size) / 256.0\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr, momentum=momentum, weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b001c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : False\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
      "  warnings.warn(msg, DeprecatedFeatureWarning)\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = amp.initialize(\n",
    "    model,\n",
    "    optimizer,\n",
    "    opt_level=opt_level,\n",
    "    keep_batchnorm_fp32=keep_batchnorm_fp32,\n",
    "    loss_scale=loss_scale,\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e5d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 224\n",
    "val_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "764bc49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = create_dali_pipeline(\n",
    "    batch_size=batch_size,\n",
    "    num_threads=workers,\n",
    "    device_id=local_rank,\n",
    "    seed=12 + local_rank,\n",
    "    data_dir=traindir,\n",
    "    crop=crop_size,\n",
    "    size=val_size,\n",
    "    dali_cpu=False,\n",
    "    shard_id=local_rank,\n",
    "    num_shards=world_size,\n",
    "    is_training=True,\n",
    ")\n",
    "pipe.build()\n",
    "train_loader = DALIClassificationIterator(\n",
    "    pipe, reader_name=\"Reader\", last_batch_policy=LastBatchPolicy.PARTIAL\n",
    ")\n",
    "\n",
    "pipe = create_dali_pipeline(\n",
    "    batch_size=batch_size,\n",
    "    num_threads=workers,\n",
    "    device_id=local_rank,\n",
    "    seed=12 + local_rank,\n",
    "    data_dir=valdir,\n",
    "    crop=crop_size,\n",
    "    size=val_size,\n",
    "    dali_cpu=False,\n",
    "    shard_id=local_rank,\n",
    "    num_shards=world_size,\n",
    "    is_training=False,\n",
    ")\n",
    "pipe.build()\n",
    "val_loader = DALIClassificationIterator(\n",
    "    pipe, reader_name=\"Reader\", last_batch_policy=LastBatchPolicy.PARTIAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4775f229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456.65%\n"
     ]
    }
   ],
   "source": [
    "a = 456.6489754575675\n",
    "print(\"{0:.2f}%\".format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0218c89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Epoch: [0][0/1]\tTime 0.130 (0.130)\tSpeed 3948.988 (3948.988)\tLoss 7.0319161415 (7.0319)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48175.782 (48175.782)\tLoss 7.0695 (7.0695)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      " * Prec@1 0.000 Prec@5 0.000\n",
      "Warning:  unscaling grads that are not FP32. Unscaling non-fp32 grads may indicate an error. When using Amp, you don't need to call .half() on your model.\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Epoch: [1][0/1]\tTime 0.084 (0.084)\tSpeed 6130.367 (6130.367)\tLoss 6.9391155243 (6.9391)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 46009.291 (46009.291)\tLoss 6.8918 (6.8918)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      " * Prec@1 0.000 Prec@5 0.000\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Epoch: [2][0/1]\tTime 0.006 (0.006)\tSpeed 80642.427 (80642.427)\tLoss 7.0344219208 (7.0344)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47808.977 (47808.977)\tLoss 6.8862 (6.8862)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      " * Prec@1 0.000 Prec@5 0.000\n",
      "Epoch: [3][0/1]\tTime 0.006 (0.006)\tSpeed 89660.796 (89660.796)\tLoss 6.9593539238 (6.9594)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48699.087 (48699.087)\tLoss 6.9347 (6.9347)\tPrec@1 8.333 (8.333)\tPrec@5 29.167 (29.167)\n",
      " * Prec@1 8.333 Prec@5 29.167\n",
      "Epoch: [4][0/1]\tTime 0.006 (0.006)\tSpeed 87510.234 (87510.234)\tLoss 6.2884583473 (6.2885)\tPrec@1 4.348 (4.348)\tPrec@5 17.391 (17.391)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49031.546 (49031.546)\tLoss 24.1746 (24.1746)\tPrec@1 4.348 (4.348)\tPrec@5 30.435 (30.435)\n",
      " * Prec@1 4.348 Prec@5 30.435\n",
      "Epoch: [5][0/1]\tTime 0.006 (0.006)\tSpeed 85460.421 (85460.421)\tLoss 4.3510551453 (4.3511)\tPrec@1 8.333 (8.333)\tPrec@5 41.667 (41.667)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47102.203 (47102.203)\tLoss 28.1608 (28.1608)\tPrec@1 4.167 (4.167)\tPrec@5 33.333 (33.333)\n",
      " * Prec@1 4.167 Prec@5 33.333\n",
      "Epoch: [6][0/1]\tTime 0.006 (0.006)\tSpeed 89695.623 (89695.623)\tLoss 4.6464424133 (4.6464)\tPrec@1 4.348 (4.348)\tPrec@5 21.739 (21.739)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49574.857 (49574.857)\tLoss 28.9942 (28.9942)\tPrec@1 8.696 (8.696)\tPrec@5 34.783 (34.783)\n",
      " * Prec@1 8.696 Prec@5 34.783\n",
      "Epoch: [7][0/1]\tTime 0.006 (0.006)\tSpeed 87304.956 (87304.956)\tLoss 8.0411882401 (8.0412)\tPrec@1 4.167 (4.167)\tPrec@5 12.500 (12.500)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48355.858 (48355.858)\tLoss 144.9180 (144.9180)\tPrec@1 8.333 (8.333)\tPrec@5 29.167 (29.167)\n",
      " * Prec@1 8.333 Prec@5 29.167\n",
      "Epoch: [8][0/1]\tTime 0.006 (0.006)\tSpeed 88508.579 (88508.579)\tLoss 7.6030769348 (7.6031)\tPrec@1 8.696 (8.696)\tPrec@5 39.130 (39.130)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50197.135 (50197.135)\tLoss 219.5544 (219.5544)\tPrec@1 0.000 (0.000)\tPrec@5 30.435 (30.435)\n",
      " * Prec@1 0.000 Prec@5 30.435\n",
      "Epoch: [9][0/1]\tTime 0.006 (0.006)\tSpeed 85617.152 (85617.152)\tLoss 10.8300371170 (10.8300)\tPrec@1 0.000 (0.000)\tPrec@5 29.167 (29.167)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49322.087 (49322.087)\tLoss 582.4479 (582.4479)\tPrec@1 4.167 (4.167)\tPrec@5 29.167 (29.167)\n",
      " * Prec@1 4.167 Prec@5 29.167\n",
      "Epoch: [10][0/1]\tTime 0.006 (0.006)\tSpeed 88699.409 (88699.409)\tLoss 11.3946590424 (11.3947)\tPrec@1 4.348 (4.348)\tPrec@5 26.087 (26.087)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50435.277 (50435.277)\tLoss 2252.2141 (2252.2141)\tPrec@1 4.348 (4.348)\tPrec@5 21.739 (21.739)\n",
      " * Prec@1 4.348 Prec@5 21.739\n",
      "Epoch: [11][0/1]\tTime 0.006 (0.006)\tSpeed 87953.590 (87953.590)\tLoss 8.2884893417 (8.2885)\tPrec@1 8.333 (8.333)\tPrec@5 37.500 (37.500)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47674.185 (47674.185)\tLoss 5926.3750 (5926.3750)\tPrec@1 4.167 (4.167)\tPrec@5 25.000 (25.000)\n",
      " * Prec@1 4.167 Prec@5 25.000\n",
      "Epoch: [12][0/1]\tTime 0.006 (0.006)\tSpeed 89550.872 (89550.872)\tLoss 11.3942623138 (11.3943)\tPrec@1 13.043 (13.043)\tPrec@5 30.435 (30.435)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50365.487 (50365.487)\tLoss 17452.8418 (17452.8418)\tPrec@1 4.348 (4.348)\tPrec@5 21.739 (21.739)\n",
      " * Prec@1 4.348 Prec@5 21.739\n",
      "Epoch: [13][0/1]\tTime 0.006 (0.006)\tSpeed 87472.450 (87472.450)\tLoss 8.5449724197 (8.5450)\tPrec@1 4.167 (4.167)\tPrec@5 25.000 (25.000)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47910.306 (47910.306)\tLoss nan (nan)\tPrec@1 8.333 (8.333)\tPrec@5 33.333 (33.333)\n",
      " * Prec@1 8.333 Prec@5 33.333\n",
      "Epoch: [14][0/1]\tTime 0.006 (0.006)\tSpeed 88900.631 (88900.631)\tLoss 10.0920677185 (10.0921)\tPrec@1 4.348 (4.348)\tPrec@5 21.739 (21.739)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50345.414 (50345.414)\tLoss nan (nan)\tPrec@1 4.348 (4.348)\tPrec@5 17.391 (17.391)\n",
      " * Prec@1 4.348 Prec@5 17.391\n",
      "Epoch: [15][0/1]\tTime 0.006 (0.006)\tSpeed 86089.086 (86089.086)\tLoss 7.1686367989 (7.1686)\tPrec@1 12.500 (12.500)\tPrec@5 37.500 (37.500)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49414.014 (49414.014)\tLoss nan (nan)\tPrec@1 0.000 (0.000)\tPrec@5 12.500 (12.500)\n",
      " * Prec@1 0.000 Prec@5 12.500\n",
      "Epoch: [16][0/1]\tTime 0.006 (0.006)\tSpeed 86239.128 (86239.128)\tLoss 7.3163418770 (7.3163)\tPrec@1 8.696 (8.696)\tPrec@5 17.391 (17.391)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 51219.588 (51219.588)\tLoss nan (nan)\tPrec@1 0.000 (0.000)\tPrec@5 4.348 (4.348)\n",
      " * Prec@1 0.000 Prec@5 4.348\n",
      "Epoch: [17][0/1]\tTime 0.006 (0.006)\tSpeed 88017.036 (88017.036)\tLoss 3.4819812775 (3.4820)\tPrec@1 25.000 (25.000)\tPrec@5 54.167 (54.167)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47759.005 (47759.005)\tLoss nan (nan)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      " * Prec@1 0.000 Prec@5 0.000\n",
      "Epoch: [18][0/1]\tTime 0.006 (0.006)\tSpeed 87732.248 (87732.248)\tLoss 5.2979888916 (5.2980)\tPrec@1 4.348 (4.348)\tPrec@5 21.739 (21.739)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50627.900 (50627.900)\tLoss nan (nan)\tPrec@1 0.000 (0.000)\tPrec@5 8.696 (8.696)\n",
      " * Prec@1 0.000 Prec@5 8.696\n",
      "Epoch: [19][0/1]\tTime 0.006 (0.006)\tSpeed 89254.976 (89254.976)\tLoss 4.3108687401 (4.3109)\tPrec@1 8.333 (8.333)\tPrec@5 58.333 (58.333)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 44529.583 (44529.583)\tLoss nan (nan)\tPrec@1 0.000 (0.000)\tPrec@5 8.333 (8.333)\n",
      " * Prec@1 0.000 Prec@5 8.333\n",
      "Epoch: [20][0/1]\tTime 0.006 (0.006)\tSpeed 89197.143 (89197.143)\tLoss 5.7694334984 (5.7694)\tPrec@1 0.000 (0.000)\tPrec@5 34.783 (34.783)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50526.649 (50526.649)\tLoss nan (nan)\tPrec@1 0.000 (0.000)\tPrec@5 8.696 (8.696)\n",
      " * Prec@1 0.000 Prec@5 8.696\n",
      "Epoch: [21][0/1]\tTime 0.006 (0.006)\tSpeed 86569.044 (86569.044)\tLoss 5.3547167778 (5.3547)\tPrec@1 8.333 (8.333)\tPrec@5 50.000 (50.000)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47471.840 (47471.840)\tLoss nan (nan)\tPrec@1 8.333 (8.333)\tPrec@5 20.833 (20.833)\n",
      " * Prec@1 8.333 Prec@5 20.833\n",
      "Epoch: [22][0/1]\tTime 0.006 (0.006)\tSpeed 88862.373 (88862.373)\tLoss 5.4043560028 (5.4044)\tPrec@1 4.348 (4.348)\tPrec@5 43.478 (43.478)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50312.388 (50312.388)\tLoss nan (nan)\tPrec@1 0.000 (0.000)\tPrec@5 26.087 (26.087)\n",
      " * Prec@1 0.000 Prec@5 26.087\n",
      "Epoch: [23][0/1]\tTime 0.006 (0.006)\tSpeed 87697.137 (87697.137)\tLoss 4.4295506477 (4.4296)\tPrec@1 8.333 (8.333)\tPrec@5 41.667 (41.667)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47520.162 (47520.162)\tLoss nan (nan)\tPrec@1 8.333 (8.333)\tPrec@5 33.333 (33.333)\n",
      " * Prec@1 8.333 Prec@5 33.333\n",
      "Epoch: [24][0/1]\tTime 0.006 (0.006)\tSpeed 87657.045 (87657.045)\tLoss 3.1835479736 (3.1835)\tPrec@1 13.043 (13.043)\tPrec@5 60.870 (60.870)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49766.718 (49766.718)\tLoss nan (nan)\tPrec@1 0.000 (0.000)\tPrec@5 30.435 (30.435)\n",
      " * Prec@1 0.000 Prec@5 30.435\n",
      "Epoch: [25][0/1]\tTime 0.006 (0.006)\tSpeed 82282.850 (82282.850)\tLoss 4.8446297646 (4.8446)\tPrec@1 4.167 (4.167)\tPrec@5 50.000 (50.000)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48709.028 (48709.028)\tLoss 2307.8757 (2307.8757)\tPrec@1 8.333 (8.333)\tPrec@5 41.667 (41.667)\n",
      " * Prec@1 8.333 Prec@5 41.667\n",
      "Epoch: [26][0/1]\tTime 0.006 (0.006)\tSpeed 88557.487 (88557.487)\tLoss 2.8873565197 (2.8874)\tPrec@1 26.087 (26.087)\tPrec@5 56.522 (56.522)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50288.824 (50288.824)\tLoss 622.5916 (622.5916)\tPrec@1 4.348 (4.348)\tPrec@5 26.087 (26.087)\n",
      " * Prec@1 4.348 Prec@5 26.087\n",
      "Epoch: [27][0/1]\tTime 0.006 (0.006)\tSpeed 89455.749 (89455.749)\tLoss 5.0491709709 (5.0492)\tPrec@1 8.333 (8.333)\tPrec@5 33.333 (33.333)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47430.949 (47430.949)\tLoss 354.8568 (354.8568)\tPrec@1 8.333 (8.333)\tPrec@5 29.167 (29.167)\n",
      " * Prec@1 8.333 Prec@5 29.167\n",
      "Epoch: [28][0/1]\tTime 0.006 (0.006)\tSpeed 90543.122 (90543.122)\tLoss 2.9885158539 (2.9885)\tPrec@1 17.391 (17.391)\tPrec@5 69.565 (69.565)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50249.992 (50249.992)\tLoss 412.2769 (412.2769)\tPrec@1 8.696 (8.696)\tPrec@5 21.739 (21.739)\n",
      " * Prec@1 8.696 Prec@5 21.739\n",
      "Epoch: [29][0/1]\tTime 0.006 (0.006)\tSpeed 86562.414 (86562.414)\tLoss 4.4594912529 (4.4595)\tPrec@1 4.167 (4.167)\tPrec@5 41.667 (41.667)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49158.376 (49158.376)\tLoss 812.7554 (812.7554)\tPrec@1 0.000 (0.000)\tPrec@5 29.167 (29.167)\n",
      " * Prec@1 0.000 Prec@5 29.167\n",
      "Epoch: [30][0/1]\tTime 0.006 (0.006)\tSpeed 88711.500 (88711.500)\tLoss 3.0480060577 (3.0480)\tPrec@1 26.087 (26.087)\tPrec@5 65.217 (65.217)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50907.540 (50907.540)\tLoss 217.3434 (217.3434)\tPrec@1 0.000 (0.000)\tPrec@5 26.087 (26.087)\n",
      " * Prec@1 0.000 Prec@5 26.087\n",
      "Epoch: [31][0/1]\tTime 0.006 (0.006)\tSpeed 88121.054 (88121.054)\tLoss 4.1191706657 (4.1192)\tPrec@1 0.000 (0.000)\tPrec@5 37.500 (37.500)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47558.048 (47558.048)\tLoss 436.8932 (436.8932)\tPrec@1 4.167 (4.167)\tPrec@5 29.167 (29.167)\n",
      " * Prec@1 4.167 Prec@5 29.167\n",
      "Epoch: [32][0/1]\tTime 0.006 (0.006)\tSpeed 87534.133 (87534.133)\tLoss 2.8864653111 (2.8865)\tPrec@1 26.087 (26.087)\tPrec@5 65.217 (65.217)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50273.519 (50273.519)\tLoss 74.4613 (74.4613)\tPrec@1 0.000 (0.000)\tPrec@5 26.087 (26.087)\n",
      " * Prec@1 0.000 Prec@5 26.087\n",
      "Epoch: [33][0/1]\tTime 0.006 (0.006)\tSpeed 88888.120 (88888.120)\tLoss 4.7783370018 (4.7783)\tPrec@1 16.667 (16.667)\tPrec@5 50.000 (50.000)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48199.570 (48199.570)\tLoss 192.9993 (192.9993)\tPrec@1 4.167 (4.167)\tPrec@5 33.333 (33.333)\n",
      " * Prec@1 4.167 Prec@5 33.333\n",
      "Epoch: [34][0/1]\tTime 0.006 (0.006)\tSpeed 90867.624 (90867.624)\tLoss 2.5740695000 (2.5741)\tPrec@1 26.087 (26.087)\tPrec@5 65.217 (65.217)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 51171.988 (51171.988)\tLoss 21.1394 (21.1394)\tPrec@1 17.391 (17.391)\tPrec@5 39.130 (39.130)\n",
      " * Prec@1 17.391 Prec@5 39.130\n",
      "Epoch: [35][0/1]\tTime 0.006 (0.006)\tSpeed 87349.700 (87349.700)\tLoss 3.8183193207 (3.8183)\tPrec@1 12.500 (12.500)\tPrec@5 58.333 (58.333)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49342.485 (49342.485)\tLoss 77.6830 (77.6830)\tPrec@1 4.167 (4.167)\tPrec@5 33.333 (33.333)\n",
      " * Prec@1 4.167 Prec@5 33.333\n",
      "Epoch: [36][0/1]\tTime 0.006 (0.006)\tSpeed 86009.438 (86009.438)\tLoss 2.4671928883 (2.4672)\tPrec@1 43.478 (43.478)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48202.816 (48202.816)\tLoss 9.1403 (9.1403)\tPrec@1 17.391 (17.391)\tPrec@5 47.826 (47.826)\n",
      " * Prec@1 17.391 Prec@5 47.826\n",
      "Epoch: [37][0/1]\tTime 0.006 (0.006)\tSpeed 85646.176 (85646.176)\tLoss 2.7337157726 (2.7337)\tPrec@1 20.833 (20.833)\tPrec@5 62.500 (62.500)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47854.789 (47854.789)\tLoss 35.5885 (35.5885)\tPrec@1 8.333 (8.333)\tPrec@5 37.500 (37.500)\n",
      " * Prec@1 8.333 Prec@5 37.500\n",
      "Epoch: [38][0/1]\tTime 0.006 (0.006)\tSpeed 90037.468 (90037.468)\tLoss 2.9220464230 (2.9220)\tPrec@1 30.435 (30.435)\tPrec@5 65.217 (65.217)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50670.906 (50670.906)\tLoss 5.1675 (5.1675)\tPrec@1 30.435 (30.435)\tPrec@5 52.174 (52.174)\n",
      " * Prec@1 30.435 Prec@5 52.174\n",
      "Epoch: [39][0/1]\tTime 0.006 (0.006)\tSpeed 86647.285 (86647.285)\tLoss 3.4892132282 (3.4892)\tPrec@1 20.833 (20.833)\tPrec@5 54.167 (54.167)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47225.467 (47225.467)\tLoss 17.2022 (17.2022)\tPrec@1 12.500 (12.500)\tPrec@5 54.167 (54.167)\n",
      " * Prec@1 12.500 Prec@5 54.167\n",
      "Epoch: [40][0/1]\tTime 0.006 (0.006)\tSpeed 92225.657 (92225.657)\tLoss 2.4262084961 (2.4262)\tPrec@1 26.087 (26.087)\tPrec@5 69.565 (69.565)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49565.703 (49565.703)\tLoss 3.3626 (3.3626)\tPrec@1 39.130 (39.130)\tPrec@5 56.522 (56.522)\n",
      " * Prec@1 39.130 Prec@5 56.522\n",
      "Epoch: [41][0/1]\tTime 0.006 (0.006)\tSpeed 91323.991 (91323.991)\tLoss 3.4312429428 (3.4312)\tPrec@1 12.500 (12.500)\tPrec@5 58.333 (58.333)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47534.888 (47534.888)\tLoss 7.7371 (7.7371)\tPrec@1 12.500 (12.500)\tPrec@5 62.500 (62.500)\n",
      " * Prec@1 12.500 Prec@5 62.500\n",
      "Epoch: [42][0/1]\tTime 0.006 (0.006)\tSpeed 89333.319 (89333.319)\tLoss 3.2499048710 (3.2499)\tPrec@1 34.783 (34.783)\tPrec@5 65.217 (65.217)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48207.144 (48207.144)\tLoss 2.6386 (2.6386)\tPrec@1 34.783 (34.783)\tPrec@5 65.217 (65.217)\n",
      " * Prec@1 34.783 Prec@5 65.217\n",
      "Epoch: [43][0/1]\tTime 0.006 (0.006)\tSpeed 87264.867 (87264.867)\tLoss 3.1426928043 (3.1427)\tPrec@1 16.667 (16.667)\tPrec@5 54.167 (54.167)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48473.740 (48473.740)\tLoss 3.9492 (3.9492)\tPrec@1 16.667 (16.667)\tPrec@5 66.667 (66.667)\n",
      " * Prec@1 16.667 Prec@5 66.667\n",
      "Epoch: [44][0/1]\tTime 0.006 (0.006)\tSpeed 90352.267 (90352.267)\tLoss 2.3366146088 (2.3366)\tPrec@1 43.478 (43.478)\tPrec@5 69.565 (69.565)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50411.598 (50411.598)\tLoss 2.4412 (2.4412)\tPrec@1 30.435 (30.435)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 30.435 Prec@5 69.565\n",
      "Epoch: [45][0/1]\tTime 0.006 (0.006)\tSpeed 89338.150 (89338.150)\tLoss 6.2447490692 (6.2447)\tPrec@1 25.000 (25.000)\tPrec@5 66.667 (66.667)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47344.158 (47344.158)\tLoss 2.8283 (2.8283)\tPrec@1 20.833 (20.833)\tPrec@5 66.667 (66.667)\n",
      " * Prec@1 20.833 Prec@5 66.667\n",
      "Epoch: [46][0/1]\tTime 0.006 (0.006)\tSpeed 88147.459 (88147.459)\tLoss 2.3589789867 (2.3590)\tPrec@1 34.783 (34.783)\tPrec@5 65.217 (65.217)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49386.741 (49386.741)\tLoss 2.3894 (2.3894)\tPrec@1 34.783 (34.783)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 34.783 Prec@5 69.565\n",
      "Epoch: [47][0/1]\tTime 0.006 (0.006)\tSpeed 89061.012 (89061.012)\tLoss 4.3849763870 (4.3850)\tPrec@1 25.000 (25.000)\tPrec@5 54.167 (54.167)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 45927.620 (45927.620)\tLoss 2.7689 (2.7689)\tPrec@1 20.833 (20.833)\tPrec@5 62.500 (62.500)\n",
      " * Prec@1 20.833 Prec@5 62.500\n",
      "Epoch: [48][0/1]\tTime 0.006 (0.006)\tSpeed 88651.075 (88651.075)\tLoss 2.3150048256 (2.3150)\tPrec@1 39.130 (39.130)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49873.050 (49873.050)\tLoss 2.4284 (2.4284)\tPrec@1 26.087 (26.087)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 26.087 Prec@5 69.565\n",
      "Epoch: [49][0/1]\tTime 0.006 (0.006)\tSpeed 86391.432 (86391.432)\tLoss 3.6615154743 (3.6615)\tPrec@1 16.667 (16.667)\tPrec@5 58.333 (58.333)\n",
      "Test: [0/0]\tTime 0.012 (0.012)\tSpeed 44297.194 (44297.194)\tLoss 2.8000 (2.8000)\tPrec@1 16.667 (16.667)\tPrec@5 62.500 (62.500)\n",
      " * Prec@1 16.667 Prec@5 62.500\n",
      "Epoch: [50][0/1]\tTime 0.006 (0.006)\tSpeed 89226.050 (89226.050)\tLoss 2.2981123924 (2.2981)\tPrec@1 39.130 (39.130)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50206.524 (50206.524)\tLoss 2.4730 (2.4730)\tPrec@1 26.087 (26.087)\tPrec@5 60.870 (60.870)\n",
      " * Prec@1 26.087 Prec@5 60.870\n",
      "Epoch: [51][0/1]\tTime 0.006 (0.006)\tSpeed 89117.559 (89117.559)\tLoss 6.0666708946 (6.0667)\tPrec@1 16.667 (16.667)\tPrec@5 45.833 (45.833)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47023.816 (47023.816)\tLoss 2.7314 (2.7314)\tPrec@1 25.000 (25.000)\tPrec@5 66.667 (66.667)\n",
      " * Prec@1 25.000 Prec@5 66.667\n",
      "Epoch: [52][0/1]\tTime 0.006 (0.006)\tSpeed 86770.522 (86770.522)\tLoss 2.3364839554 (2.3365)\tPrec@1 39.130 (39.130)\tPrec@5 69.565 (69.565)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50925.648 (50925.648)\tLoss 2.2194 (2.2194)\tPrec@1 34.783 (34.783)\tPrec@5 65.217 (65.217)\n",
      " * Prec@1 34.783 Prec@5 65.217\n",
      "Epoch: [53][0/1]\tTime 0.006 (0.006)\tSpeed 87216.666 (87216.666)\tLoss 4.3568644524 (4.3569)\tPrec@1 20.833 (20.833)\tPrec@5 62.500 (62.500)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47104.270 (47104.270)\tLoss 3.9794 (3.9794)\tPrec@1 29.167 (29.167)\tPrec@5 66.667 (66.667)\n",
      " * Prec@1 29.167 Prec@5 66.667\n",
      "Epoch: [54][0/1]\tTime 0.006 (0.006)\tSpeed 89374.587 (89374.587)\tLoss 2.3535053730 (2.3535)\tPrec@1 39.130 (39.130)\tPrec@5 78.261 (78.261)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50269.989 (50269.989)\tLoss 2.3510 (2.3510)\tPrec@1 26.087 (26.087)\tPrec@5 73.913 (73.913)\n",
      " * Prec@1 26.087 Prec@5 73.913\n",
      "Epoch: [55][0/1]\tTime 0.006 (0.006)\tSpeed 87716.839 (87716.839)\tLoss 3.0614147186 (3.0614)\tPrec@1 25.000 (25.000)\tPrec@5 54.167 (54.167)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49062.912 (49062.912)\tLoss 4.6581 (4.6581)\tPrec@1 25.000 (25.000)\tPrec@5 50.000 (50.000)\n",
      " * Prec@1 25.000 Prec@5 50.000\n",
      "Epoch: [56][0/1]\tTime 0.006 (0.006)\tSpeed 88481.957 (88481.957)\tLoss 2.2056984901 (2.2057)\tPrec@1 34.783 (34.783)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49968.207 (49968.207)\tLoss 2.3921 (2.3921)\tPrec@1 26.087 (26.087)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 26.087 Prec@5 69.565\n",
      "Epoch: [57][0/1]\tTime 0.006 (0.006)\tSpeed 86493.382 (86493.382)\tLoss 2.7369661331 (2.7370)\tPrec@1 16.667 (16.667)\tPrec@5 45.833 (45.833)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48327.564 (48327.564)\tLoss 4.5689 (4.5689)\tPrec@1 16.667 (16.667)\tPrec@5 45.833 (45.833)\n",
      " * Prec@1 16.667 Prec@5 45.833\n",
      "Epoch: [58][0/1]\tTime 0.006 (0.006)\tSpeed 87014.171 (87014.171)\tLoss 2.2648792267 (2.2649)\tPrec@1 39.130 (39.130)\tPrec@5 65.217 (65.217)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49571.424 (49571.424)\tLoss 2.3909 (2.3909)\tPrec@1 34.783 (34.783)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 34.783 Prec@5 69.565\n",
      "Epoch: [59][0/1]\tTime 0.006 (0.006)\tSpeed 89496.385 (89496.385)\tLoss 2.6577329636 (2.6577)\tPrec@1 33.333 (33.333)\tPrec@5 62.500 (62.500)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47962.738 (47962.738)\tLoss 4.7280 (4.7280)\tPrec@1 8.333 (8.333)\tPrec@5 50.000 (50.000)\n",
      " * Prec@1 8.333 Prec@5 50.000\n",
      "Epoch: [60][0/1]\tTime 0.006 (0.006)\tSpeed 88927.137 (88927.137)\tLoss 2.4448008537 (2.4448)\tPrec@1 26.087 (26.087)\tPrec@5 60.870 (60.870)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50788.346 (50788.346)\tLoss 2.2605 (2.2605)\tPrec@1 34.783 (34.783)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 34.783 Prec@5 69.565\n",
      "Epoch: [61][0/1]\tTime 0.006 (0.006)\tSpeed 87849.967 (87849.967)\tLoss 2.8608887196 (2.8609)\tPrec@1 29.167 (29.167)\tPrec@5 54.167 (54.167)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48448.588 (48448.588)\tLoss 3.9218 (3.9218)\tPrec@1 12.500 (12.500)\tPrec@5 54.167 (54.167)\n",
      " * Prec@1 12.500 Prec@5 54.167\n",
      "Epoch: [62][0/1]\tTime 0.006 (0.006)\tSpeed 88728.728 (88728.728)\tLoss 2.2334568501 (2.2335)\tPrec@1 39.130 (39.130)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50172.507 (50172.507)\tLoss 2.1301 (2.1301)\tPrec@1 39.130 (39.130)\tPrec@5 73.913 (73.913)\n",
      " * Prec@1 39.130 Prec@5 73.913\n",
      "Epoch: [63][0/1]\tTime 0.006 (0.006)\tSpeed 85016.198 (85016.198)\tLoss 3.4149608612 (3.4150)\tPrec@1 33.333 (33.333)\tPrec@5 54.167 (54.167)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48463.895 (48463.895)\tLoss 3.1814 (3.1814)\tPrec@1 29.167 (29.167)\tPrec@5 66.667 (66.667)\n",
      " * Prec@1 29.167 Prec@5 66.667\n",
      "Epoch: [64][0/1]\tTime 0.006 (0.006)\tSpeed 89378.307 (89378.307)\tLoss 2.0022411346 (2.0022)\tPrec@1 43.478 (43.478)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 48798.683 (48798.683)\tLoss 2.0787 (2.0787)\tPrec@1 39.130 (39.130)\tPrec@5 73.913 (73.913)\n",
      " * Prec@1 39.130 Prec@5 73.913\n",
      "Epoch: [65][0/1]\tTime 0.006 (0.006)\tSpeed 85025.286 (85025.286)\tLoss 2.5424621105 (2.5425)\tPrec@1 33.333 (33.333)\tPrec@5 58.333 (58.333)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49022.592 (49022.592)\tLoss 2.8400 (2.8400)\tPrec@1 37.500 (37.500)\tPrec@5 70.833 (70.833)\n",
      " * Prec@1 37.500 Prec@5 70.833\n",
      "Epoch: [66][0/1]\tTime 0.006 (0.006)\tSpeed 87758.062 (87758.062)\tLoss 2.0502045155 (2.0502)\tPrec@1 43.478 (43.478)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49994.963 (49994.963)\tLoss 2.0614 (2.0614)\tPrec@1 34.783 (34.783)\tPrec@5 73.913 (73.913)\n",
      " * Prec@1 34.783 Prec@5 73.913\n",
      "Epoch: [67][0/1]\tTime 0.006 (0.006)\tSpeed 87131.030 (87131.030)\tLoss 2.4981224537 (2.4981)\tPrec@1 25.000 (25.000)\tPrec@5 66.667 (66.667)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48316.691 (48316.691)\tLoss 2.6382 (2.6382)\tPrec@1 33.333 (33.333)\tPrec@5 70.833 (70.833)\n",
      " * Prec@1 33.333 Prec@5 70.833\n",
      "Epoch: [68][0/1]\tTime 0.006 (0.006)\tSpeed 88635.342 (88635.342)\tLoss 2.1816112995 (2.1816)\tPrec@1 30.435 (30.435)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49697.615 (49697.615)\tLoss 2.0845 (2.0845)\tPrec@1 34.783 (34.783)\tPrec@5 78.261 (78.261)\n",
      " * Prec@1 34.783 Prec@5 78.261\n",
      "Epoch: [69][0/1]\tTime 0.006 (0.006)\tSpeed 88364.358 (88364.358)\tLoss 2.6280839443 (2.6281)\tPrec@1 45.833 (45.833)\tPrec@5 58.333 (58.333)\n",
      "Test: [0/0]\tTime 0.012 (0.012)\tSpeed 42366.707 (42366.707)\tLoss 2.5533 (2.5533)\tPrec@1 33.333 (33.333)\tPrec@5 70.833 (70.833)\n",
      " * Prec@1 33.333 Prec@5 70.833\n",
      "Epoch: [70][0/1]\tTime 0.006 (0.006)\tSpeed 89353.762 (89353.762)\tLoss 2.3476793766 (2.3477)\tPrec@1 26.087 (26.087)\tPrec@5 60.870 (60.870)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48674.803 (48674.803)\tLoss 2.1006 (2.1006)\tPrec@1 30.435 (30.435)\tPrec@5 78.261 (78.261)\n",
      " * Prec@1 30.435 Prec@5 78.261\n",
      "Epoch: [71][0/1]\tTime 0.006 (0.006)\tSpeed 84483.072 (84483.072)\tLoss 2.4080624580 (2.4081)\tPrec@1 29.167 (29.167)\tPrec@5 66.667 (66.667)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49409.467 (49409.467)\tLoss 2.4943 (2.4943)\tPrec@1 25.000 (25.000)\tPrec@5 70.833 (70.833)\n",
      " * Prec@1 25.000 Prec@5 70.833\n",
      "Epoch: [72][0/1]\tTime 0.006 (0.006)\tSpeed 88056.736 (88056.736)\tLoss 2.1633813381 (2.1634)\tPrec@1 39.130 (39.130)\tPrec@5 69.565 (69.565)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48284.100 (48284.100)\tLoss 2.1107 (2.1107)\tPrec@1 30.435 (30.435)\tPrec@5 78.261 (78.261)\n",
      " * Prec@1 30.435 Prec@5 78.261\n",
      "Epoch: [73][0/1]\tTime 0.006 (0.006)\tSpeed 88315.299 (88315.299)\tLoss 2.4626600742 (2.4627)\tPrec@1 50.000 (50.000)\tPrec@5 66.667 (66.667)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 46654.001 (46654.001)\tLoss 2.4814 (2.4814)\tPrec@1 25.000 (25.000)\tPrec@5 75.000 (75.000)\n",
      " * Prec@1 25.000 Prec@5 75.000\n",
      "Epoch: [74][0/1]\tTime 0.006 (0.006)\tSpeed 90553.430 (90553.430)\tLoss 2.2593023777 (2.2593)\tPrec@1 30.435 (30.435)\tPrec@5 60.870 (60.870)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50092.924 (50092.924)\tLoss 2.1267 (2.1267)\tPrec@1 30.435 (30.435)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 30.435 Prec@5 69.565\n",
      "Epoch: [75][0/1]\tTime 0.006 (0.006)\tSpeed 90672.338 (90672.338)\tLoss 3.0273034573 (3.0273)\tPrec@1 29.167 (29.167)\tPrec@5 58.333 (58.333)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 46795.312 (46795.312)\tLoss 2.4863 (2.4863)\tPrec@1 25.000 (25.000)\tPrec@5 70.833 (70.833)\n",
      " * Prec@1 25.000 Prec@5 70.833\n",
      "Epoch: [76][0/1]\tTime 0.006 (0.006)\tSpeed 89972.962 (89972.962)\tLoss 2.1554877758 (2.1555)\tPrec@1 30.435 (30.435)\tPrec@5 78.261 (78.261)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50122.153 (50122.153)\tLoss 2.1422 (2.1422)\tPrec@1 30.435 (30.435)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 30.435 Prec@5 69.565\n",
      "Epoch: [77][0/1]\tTime 0.006 (0.006)\tSpeed 86979.985 (86979.985)\tLoss 2.6591174603 (2.6591)\tPrec@1 37.500 (37.500)\tPrec@5 75.000 (75.000)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48617.501 (48617.501)\tLoss 2.4772 (2.4772)\tPrec@1 25.000 (25.000)\tPrec@5 70.833 (70.833)\n",
      " * Prec@1 25.000 Prec@5 70.833\n",
      "Epoch: [78][0/1]\tTime 0.006 (0.006)\tSpeed 91564.633 (91564.633)\tLoss 2.2315993309 (2.2316)\tPrec@1 43.478 (43.478)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50444.755 (50444.755)\tLoss 2.1433 (2.1433)\tPrec@1 34.783 (34.783)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 34.783 Prec@5 69.565\n",
      "Epoch: [79][0/1]\tTime 0.006 (0.006)\tSpeed 91854.847 (91854.847)\tLoss 2.6341283321 (2.6341)\tPrec@1 29.167 (29.167)\tPrec@5 58.333 (58.333)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 46640.828 (46640.828)\tLoss 2.4801 (2.4801)\tPrec@1 25.000 (25.000)\tPrec@5 66.667 (66.667)\n",
      " * Prec@1 25.000 Prec@5 66.667\n",
      "Epoch: [80][0/1]\tTime 0.005 (0.005)\tSpeed 93253.706 (93253.706)\tLoss 2.2074661255 (2.2075)\tPrec@1 30.435 (30.435)\tPrec@5 69.565 (69.565)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50290.002 (50290.002)\tLoss 2.1516 (2.1516)\tPrec@1 34.783 (34.783)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 34.783 Prec@5 69.565\n",
      "Epoch: [81][0/1]\tTime 0.006 (0.006)\tSpeed 89057.688 (89057.688)\tLoss 2.6977908611 (2.6978)\tPrec@1 20.833 (20.833)\tPrec@5 54.167 (54.167)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 46948.769 (46948.769)\tLoss 2.4768 (2.4768)\tPrec@1 25.000 (25.000)\tPrec@5 70.833 (70.833)\n",
      " * Prec@1 25.000 Prec@5 70.833\n",
      "Epoch: [82][0/1]\tTime 0.006 (0.006)\tSpeed 89546.765 (89546.765)\tLoss 2.7315332890 (2.7315)\tPrec@1 30.435 (30.435)\tPrec@5 56.522 (56.522)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50268.812 (50268.812)\tLoss 2.1443 (2.1443)\tPrec@1 34.783 (34.783)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 34.783 Prec@5 69.565\n",
      "Epoch: [83][0/1]\tTime 0.006 (0.006)\tSpeed 88453.530 (88453.530)\tLoss 2.5653142929 (2.5653)\tPrec@1 29.167 (29.167)\tPrec@5 62.500 (62.500)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 48869.755 (48869.755)\tLoss 2.4844 (2.4844)\tPrec@1 25.000 (25.000)\tPrec@5 66.667 (66.667)\n",
      " * Prec@1 25.000 Prec@5 66.667\n",
      "Epoch: [84][0/1]\tTime 0.006 (0.006)\tSpeed 86342.108 (86342.108)\tLoss 2.1195707321 (2.1196)\tPrec@1 34.783 (34.783)\tPrec@5 78.261 (78.261)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50080.074 (50080.074)\tLoss 2.1565 (2.1565)\tPrec@1 30.435 (30.435)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 30.435 Prec@5 69.565\n",
      "Epoch: [85][0/1]\tTime 0.006 (0.006)\tSpeed 86993.727 (86993.727)\tLoss 2.4395015240 (2.4395)\tPrec@1 33.333 (33.333)\tPrec@5 75.000 (75.000)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47880.396 (47880.396)\tLoss 2.4931 (2.4931)\tPrec@1 25.000 (25.000)\tPrec@5 66.667 (66.667)\n",
      " * Prec@1 25.000 Prec@5 66.667\n",
      "Epoch: [86][0/1]\tTime 0.006 (0.006)\tSpeed 88778.614 (88778.614)\tLoss 2.1416659355 (2.1417)\tPrec@1 39.130 (39.130)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49953.097 (49953.097)\tLoss 2.1623 (2.1623)\tPrec@1 30.435 (30.435)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 30.435 Prec@5 69.565\n",
      "Epoch: [87][0/1]\tTime 0.006 (0.006)\tSpeed 85908.624 (85908.624)\tLoss 2.5146605968 (2.5147)\tPrec@1 41.667 (41.667)\tPrec@5 58.333 (58.333)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48298.218 (48298.218)\tLoss 2.4719 (2.4719)\tPrec@1 25.000 (25.000)\tPrec@5 66.667 (66.667)\n",
      " * Prec@1 25.000 Prec@5 66.667\n",
      "Epoch: [88][0/1]\tTime 0.006 (0.006)\tSpeed 87305.311 (87305.311)\tLoss 2.1649534702 (2.1650)\tPrec@1 34.783 (34.783)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 49263.251 (49263.251)\tLoss 2.1547 (2.1547)\tPrec@1 26.087 (26.087)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 26.087 Prec@5 69.565\n",
      "Epoch: [89][0/1]\tTime 0.006 (0.006)\tSpeed 87386.311 (87386.311)\tLoss 5.6804771423 (5.6805)\tPrec@1 37.500 (37.500)\tPrec@5 66.667 (66.667)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48279.758 (48279.758)\tLoss 2.4829 (2.4829)\tPrec@1 29.167 (29.167)\tPrec@5 66.667 (66.667)\n",
      " * Prec@1 29.167 Prec@5 66.667\n",
      "Epoch: [90][0/1]\tTime 0.006 (0.006)\tSpeed 88165.191 (88165.191)\tLoss 2.1512660980 (2.1513)\tPrec@1 34.783 (34.783)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50814.785 (50814.785)\tLoss 2.1513 (2.1513)\tPrec@1 30.435 (30.435)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 30.435 Prec@5 69.565\n",
      "Epoch: [91][0/1]\tTime 0.006 (0.006)\tSpeed 84700.996 (84700.996)\tLoss 2.6333577633 (2.6334)\tPrec@1 37.500 (37.500)\tPrec@5 62.500 (62.500)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47070.198 (47070.198)\tLoss 2.4734 (2.4734)\tPrec@1 29.167 (29.167)\tPrec@5 70.833 (70.833)\n",
      " * Prec@1 29.167 Prec@5 70.833\n",
      "Epoch: [92][0/1]\tTime 0.006 (0.006)\tSpeed 87080.506 (87080.506)\tLoss 2.1440570354 (2.1441)\tPrec@1 26.087 (26.087)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50606.425 (50606.425)\tLoss 2.1634 (2.1634)\tPrec@1 26.087 (26.087)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 26.087 Prec@5 69.565\n",
      "Epoch: [93][0/1]\tTime 0.006 (0.006)\tSpeed 90443.594 (90443.594)\tLoss 5.8871855736 (5.8872)\tPrec@1 33.333 (33.333)\tPrec@5 62.500 (62.500)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 46893.409 (46893.409)\tLoss 2.4763 (2.4763)\tPrec@1 29.167 (29.167)\tPrec@5 70.833 (70.833)\n",
      " * Prec@1 29.167 Prec@5 70.833\n",
      "Epoch: [94][0/1]\tTime 0.006 (0.006)\tSpeed 90153.131 (90153.131)\tLoss 2.1550769806 (2.1551)\tPrec@1 34.783 (34.783)\tPrec@5 73.913 (73.913)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48227.714 (48227.714)\tLoss 2.1711 (2.1711)\tPrec@1 26.087 (26.087)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 26.087 Prec@5 69.565\n",
      "Epoch: [95][0/1]\tTime 0.006 (0.006)\tSpeed 87305.311 (87305.311)\tLoss 2.4313974380 (2.4314)\tPrec@1 29.167 (29.167)\tPrec@5 66.667 (66.667)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 48517.547 (48517.547)\tLoss 2.4823 (2.4823)\tPrec@1 29.167 (29.167)\tPrec@5 70.833 (70.833)\n",
      " * Prec@1 29.167 Prec@5 70.833\n",
      "Epoch: [96][0/1]\tTime 0.006 (0.006)\tSpeed 88626.197 (88626.197)\tLoss 2.2364203930 (2.2364)\tPrec@1 34.783 (34.783)\tPrec@5 69.565 (69.565)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 50343.054 (50343.054)\tLoss 2.1771 (2.1771)\tPrec@1 26.087 (26.087)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 26.087 Prec@5 69.565\n",
      "Epoch: [97][0/1]\tTime 0.006 (0.006)\tSpeed 89530.338 (89530.338)\tLoss 2.4283385277 (2.4283)\tPrec@1 33.333 (33.333)\tPrec@5 66.667 (66.667)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 48799.792 (48799.792)\tLoss 2.4808 (2.4808)\tPrec@1 29.167 (29.167)\tPrec@5 70.833 (70.833)\n",
      " * Prec@1 29.167 Prec@5 70.833\n",
      "Epoch: [98][0/1]\tTime 0.006 (0.006)\tSpeed 86778.236 (86778.236)\tLoss 2.3194942474 (2.3195)\tPrec@1 26.087 (26.087)\tPrec@5 69.565 (69.565)\n",
      "Test: [0/0]\tTime 0.010 (0.010)\tSpeed 48968.934 (48968.934)\tLoss 2.1713 (2.1713)\tPrec@1 26.087 (26.087)\tPrec@5 69.565 (69.565)\n",
      " * Prec@1 26.087 Prec@5 69.565\n",
      "Epoch: [99][0/1]\tTime 0.006 (0.006)\tSpeed 87561.616 (87561.616)\tLoss 2.8163692951 (2.8164)\tPrec@1 41.667 (41.667)\tPrec@5 58.333 (58.333)\n",
      "Test: [0/0]\tTime 0.011 (0.011)\tSpeed 47983.100 (47983.100)\tLoss 2.4888 (2.4888)\tPrec@1 29.167 (29.167)\tPrec@5 70.833 (70.833)\n",
      " * Prec@1 29.167 Prec@5 70.833\n",
      "##Top-1 29.17%\n",
      "##Top-5 70.83%\n",
      "##Perf  65,405.80\n"
     ]
    }
   ],
   "source": [
    "total_time = AverageMeter()\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    # train for one epoch\n",
    "    avg_train_time = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    total_time.update(avg_train_time)\n",
    "    if test:\n",
    "        break\n",
    "\n",
    "    # evaluate on validation set\n",
    "    [prec1, prec5] = validate(val_loader, model, criterion)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    if local_rank == 0:\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"arch\": arch,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"best_prec1\": best_prec1,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            },\n",
    "            is_best,\n",
    "        )\n",
    "        if epoch == epochs - 1:\n",
    "            print(\n",
    "                \"##Top-1 {0:.2f}%\\n\"\n",
    "                \"##Top-5 {1:.2f}%\\n\"\n",
    "                \"##Perf  {2:,.2f}\".format(\n",
    "                    prec1, prec5, total_batch_size / total_time.avg\n",
    "                )\n",
    "            )\n",
    "\n",
    "    train_loader.reset()\n",
    "    val_loader.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c8deb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
