{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce96962",
   "metadata": {},
   "source": [
    "# NVIDIA GTC 2023\n",
    "## Developer Breakout\n",
    "### Accelerating Enterprise Workflows With Triton Server and DALI \n",
    "*[SE52140]*\n",
    "\n",
    "## Overview\n",
    "\n",
    "NVIDIA Data Loading Library (DALI) is a collection of highly optimized building blocks and an execution engine that accelerates the data pipeline for computer vision and audio deep learning applications.\n",
    "\n",
    "Input and augmentation pipelines provided by Deep Learning frameworks fit typically into one of two categories:\n",
    "\n",
    "* fast, but inflexible - written in C++, they are exposed as a single monolithic Python object with very specific set and ordering of operations it provides\n",
    "* slow, but flexible - set of building blocks written in either C++ or Python, that can be used to compose arbitrary data pipelines that end up being slow. One of the biggest overheads for this type of data pipelines is Global Interpreter Lock (GIL) in Python. This forces developers to use multiprocessing, complicating the design of efficient input pipelines.\n",
    " \n",
    "DALI stands out by providing both performance and flexibility of accelerating different data pipelines. It achieves that by exposing optimized building blocks which are executed using simple and efficient engine, and enabling offloading of operations to GPU (thus enabling scaling to multi-GPU systems).\n",
    "\n",
    "It is a single library, that can be easily integrated into different deep learning training and inference applications.\n",
    "\n",
    "DALI offers ease-of-use and flexibility across GPU enabled systems with direct framework plugins, multiple input data formats, and configurable graphs. DALI can help achieve overall speedup on deep learning workflows that are bottlenecked on I/O pipelines due to the limitations of CPU cycles. Typically, systems with high GPU to CPU ratio are constrained on the host CPU, thereby under-utilizing the available GPU compute capabilities. DALI significantly accelerates input processing on such dense GPU configurations to achieve the overall throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali import fn, math, ops, pipeline_def, types\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import psutil\n",
    "from matplotlib import gridspec, patches\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b61250",
   "metadata": {},
   "source": [
    "___\n",
    "## Pipeline\n",
    "\n",
    "At the core of data processing with DALI lies the concept of a data processing pipeline. It is composed of multiple operations connected in a directed graph and contained in an object of class class `nvidia.dali.Pipeline`. This class provides functions necessary for defining, building and running data processing pipelines.\n",
    "\n",
    "Let us start with defining a very simple pipeline for a classification task determining whether a picture contains a dog or a kitten. We prepared a directory structure containing COCO data.\n",
    "\n",
    "Our simple pipeline will read images from this directory, decode them and return (image, label) pairs.\n",
    "\n",
    "The easiest way to create a pipieline is by using the `pipeline_def` decorator. In the `simple_pipeline` function we define the operations to be performed and the flow of the computation between them.\n",
    "\n",
    "1. Use `fn.readers.coco` to read jpegs (encoded images) and labels.\n",
    "\n",
    "2. Use the `fn.decoders.image` operation to decode images from jpeg to RGB.\n",
    "\n",
    "3. Specify which of the intermediate variables should be returned as the outputs of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e452134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(image_batch, title=None, index=0, dpi=60):\n",
    "    fig, ax = plt.subplots(dpi=dpi)\n",
    "    ax.imshow(image_batch[index])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e733d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_reader_def(ratio=False):\n",
    "    inputs, bboxes, labels, polygons, vertices = fn.readers.coco(\n",
    "        file_root=coco_file_root,\n",
    "        annotations_file=coco_annotations_file,\n",
    "        polygon_masks=True,  # Load segmentation mask data as polygons\n",
    "        ratio=ratio,  # Bounding box and mask polygons to be expressed in relative coordinates\n",
    "        ltrb=True,  # Bounding boxes to be expressed as left, top, right, bottom coordinates\n",
    "    )\n",
    "    return inputs, bboxes, labels, polygons, vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71107d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speedtest(pipeline, batch, n_threads, device_id=0):\n",
    "    pipe = pipeline(batch_size=batch, num_threads=n_threads, device_id=device_id)\n",
    "    pipe.build()\n",
    "    # warmup\n",
    "    for i in range(5):\n",
    "        pipe.run()\n",
    "    # test\n",
    "    n_test = 10\n",
    "    t_start = timer()\n",
    "    for i in range(n_test):\n",
    "        pipe.run()\n",
    "    t = timer() - t_start\n",
    "    return \"{:,.0f} items/s\".format((n_test * batch) / t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beedd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def\n",
    "def cpu_pipeline():\n",
    "    encoded, bboxes, labels, polygons, vertices = coco_reader_def()\n",
    "    images = fn.decoders.image(encoded, device=\"cpu\")\n",
    "    return images\n",
    "\n",
    "\n",
    "@pipeline_def\n",
    "def gpu_pipeline():\n",
    "    encoded, bboxes, labels, polygons, vertices = coco_reader_def()\n",
    "    images = fn.decoders.image(encoded, device=\"mixed\", hw_decoder_load=0.75)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3185e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def\n",
    "def cpu_augmentation_pipeline():\n",
    "    encoded, bboxes, labels, polygons, vertices = coco_reader_def()\n",
    "    original = fn.decoders.image(encoded, device=\"cpu\")\n",
    "    sliced = fn.slice(\n",
    "        original, rel_start=[0.3, 0.2], rel_shape=[0.5, 0.6], axis_names=\"HW\"\n",
    "    )\n",
    "    sphered = fn.sphere(original)\n",
    "    rotated = fn.rotate(original, angle=30)\n",
    "    warped = fn.warp_affine(original, matrix=[1.0, 0.8, 0.0, 0.0, 1.2, 0.0])\n",
    "    hflip = fn.flip(original, vertical=0, horizontal=1)\n",
    "    bced = fn.brightness_contrast(original, brightness=0.5, contrast=1.5)\n",
    "    dist = fn.jpeg_compression_distortion(original, quality=5)\n",
    "    water = fn.water(original)\n",
    "\n",
    "    return original, sliced, sphered, rotated, warped, hflip, bced, dist, water\n",
    "\n",
    "\n",
    "@pipeline_def\n",
    "def gpu_augmentation_pipeline():\n",
    "    encoded, bboxes, labels, polygons, vertices = coco_reader_def()\n",
    "    original = fn.decoders.image(encoded, device=\"mixed\", hw_decoder_load=0.75)\n",
    "    sliced = fn.slice(\n",
    "        original, rel_start=[0.3, 0.2], rel_shape=[0.5, 0.6], axis_names=\"HW\"\n",
    "    )\n",
    "    sphered = fn.sphere(original)\n",
    "    rotated = fn.rotate(original, angle=30)\n",
    "    warped = fn.warp_affine(original, matrix=[1.0, 0.8, 0.0, 0.0, 1.2, 0.0])\n",
    "    hflip = fn.flip(original, vertical=0, horizontal=1)\n",
    "    bced = fn.brightness_contrast(original, brightness=0.5, contrast=1.5)\n",
    "    dist = fn.jpeg_compression_distortion(original, quality=5)\n",
    "    water = fn.water(original)\n",
    "\n",
    "    return original, sliced, sphered, rotated, warped, hflip, bced, dist, water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bfc0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dali_extra_dir = \"/data\"\n",
    "coco_file_root = \"/data/db/coco/images\"\n",
    "coco_annotations_file = \"/data/db/coco/instances.json\"\n",
    "batch_size = 4\n",
    "seed = 42\n",
    "initial_fill = 12\n",
    "n_threads = psutil.cpu_count()\n",
    "dpi = 130\n",
    "\n",
    "run_speedtest = True\n",
    "speedtest_batch_size = psutil.cpu_count(logical=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e9d5a",
   "metadata": {},
   "source": [
    "___\n",
    "## COCO Reader with Augmentations\n",
    "\n",
    "Here is an example that demonstrates how to combine the COCO Reader, which loads data from a COCO dataset, with some of the typical augmentations used in image detection and segmentation use cases. The COCO dataset consists of a directory with images and an annotations file containing information about bounding boxes, labels and segmentation masks.\n",
    "\n",
    "A typical augmentation applied in detection and segmentation use cases is a random crop of the image with the restriction that at least one ground truth box is present in the cropped image. In DALI, we use RandomBBoxCrop for that. RandomBBoxCrop operator takes as an input the bounding boxes and the labels associated with them, and a set of constraints for the cropping operation. The result are the cropping window anchor and shape, as well as the processed bounding boxes and labels. The anchor and shape outputs, expressed in relative coordinates, can be directly fed into DALI's Slice operator to extract the region of interest of the image. The output bounding boxes and labels are processed to contain only the ones within the cropping window, and the coordinates are mapped to the new coordinate space. RandomBBoxCrop does not process segmentation masks, so the mask coordinates need to be mapped to the new coordinate space separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_augmentation_pipe = gpu_augmentation_pipeline(\n",
    "    batch_size=batch_size, num_threads=n_threads, device_id=0, seed=seed\n",
    ")\n",
    "gpu_augmentation_pipe.build()\n",
    "(\n",
    "    original,\n",
    "    sliced,\n",
    "    sphered,\n",
    "    rotated,\n",
    "    warped,\n",
    "    hflip,\n",
    "    bced,\n",
    "    dist,\n",
    "    water,\n",
    ") = gpu_augmentation_pipe.run()\n",
    "\n",
    "image_augmentations = {\n",
    "    \"original\": original,\n",
    "    \"sliced\": sliced,\n",
    "    \"sphered\": sphered,\n",
    "    \"rotated\": rotated,\n",
    "    \"warped\": warped,\n",
    "    \"horizontal flip\": hflip,\n",
    "    \"brightness & contrast\": bced,\n",
    "    \"jpg distortion\": dist,\n",
    "    \"water\": water,\n",
    "}\n",
    "\n",
    "for k, v in image_augmentations.items():\n",
    "    show_images(v.as_cpu(), title=k, index=3, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_speedtest:\n",
    "    pipelines_speedtest = {\n",
    "        \"cpu_pipeline\": cpu_pipeline,\n",
    "        \"gpu_pipeline\": gpu_pipeline,\n",
    "        \"cpu_augmentation_pipeline\": cpu_augmentation_pipeline,\n",
    "        \"gpu_augmentation_pipeline\": gpu_augmentation_pipeline,\n",
    "    }\n",
    "\n",
    "    for k, v in pipelines_speedtest.items():\n",
    "        print(\n",
    "            \"{}: {}\".format(\n",
    "                k, speedtest(v, speedtest_batch_size, n_threads, device_id=1)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8349551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coco_sample(\n",
    "    image,\n",
    "    bboxes,\n",
    "    labels,\n",
    "    mask_polygons,\n",
    "    mask_vertices,\n",
    "    relative_coords=False,\n",
    "    title=None,\n",
    "    dpi=60,\n",
    "):\n",
    "    H, W = image.shape[0], image.shape[1]\n",
    "    fig, ax = plt.subplots(dpi=dpi)\n",
    "    ax.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "    # Bounding boxes\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        l, t, r, b = bbox * [W, H, W, H] if relative_coords else bbox\n",
    "        rect = patches.Rectangle(\n",
    "            (l, t),\n",
    "            width=(r - l),\n",
    "            height=(b - t),\n",
    "            linewidth=1,\n",
    "            edgecolor=\"#76b900\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Segmentation masks\n",
    "    for polygon in mask_polygons:\n",
    "        mask_idx, start_vertex, end_vertex = polygon\n",
    "        polygon_vertices = mask_vertices[\n",
    "            start_vertex:end_vertex\n",
    "        ]  # Select polygon vertices\n",
    "        # Scale relative coordinates to the image dimensions, if necessary\n",
    "        polygon_vertices = (\n",
    "            polygon_vertices * [W, H] if relative_coords else polygon_vertices\n",
    "        )\n",
    "        poly = patches.Polygon(\n",
    "            xy=polygon_vertices, closed=True, facecolor=\"#76b900\", alpha=0.7\n",
    "        )\n",
    "        ax.add_patch(poly)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show(outputs, relative_coords=False, title=None, index=0, dpi=60):\n",
    "    images, bboxes, labels, mask_polygons, mask_vertices = outputs\n",
    "    plot_coco_sample(\n",
    "        images.as_cpu().at(index),\n",
    "        bboxes.at(index),\n",
    "        labels.at(index),\n",
    "        mask_polygons.at(index),\n",
    "        mask_vertices.at(index),\n",
    "        relative_coords=relative_coords,\n",
    "        title=title,\n",
    "        dpi=dpi,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def\n",
    "def coco_pipeline():\n",
    "    encoded, bboxes, labels, polygons, vertices = coco_reader_def()\n",
    "    images = fn.decoders.image(\n",
    "        encoded, device=\"mixed\", output_type=types.RGB, hw_decoder_load=0.75\n",
    "    )\n",
    "    return images, bboxes, labels, polygons, vertices\n",
    "\n",
    "\n",
    "coco_pipe = coco_pipeline(\n",
    "    batch_size=batch_size, num_threads=n_threads, device_id=0, seed=seed\n",
    ")\n",
    "coco_pipe.build()\n",
    "outputs = coco_pipe.run()\n",
    "\n",
    "show(outputs, title=\"coco_pipeline\", index=3, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3868c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def\n",
    "def coco_pipeline_bbox_flip():\n",
    "    encoded, bboxes, labels, polygons, vertices = coco_reader_def(ratio=True)\n",
    "    orig_shape = fn.peek_image_shape(encoded)\n",
    "    images = fn.decoders.image(\n",
    "        encoded, device=\"mixed\", output_type=types.RGB, hw_decoder_load=0.75\n",
    "    )\n",
    "    px = fn.random.uniform(range=(0, 1))\n",
    "    py = fn.random.uniform(range=(0, 1))\n",
    "    ratio = fn.random.uniform(range=(1, 2))\n",
    "    images = fn.paste(images, paste_x=px, paste_y=py, ratio=ratio, fill_value=(0, 0, 0))\n",
    "    bboxes = fn.bbox_paste(bboxes, paste_x=px, paste_y=py, ratio=ratio, ltrb=True)\n",
    "\n",
    "    scale = 1.0 / ratio\n",
    "    margin = ratio - 1.0\n",
    "    px_1 = scale * px * margin\n",
    "    py_1 = scale * py * margin\n",
    "    ver_x = scale * fn.slice(vertices, 0, 1, axes=[1]) + px_1\n",
    "    ver_y = scale * fn.slice(vertices, 1, 1, axes=[1]) + py_1\n",
    "    vertices = fn.cat(ver_x, ver_y, axis=1)\n",
    "\n",
    "    should_flip = fn.random.coin_flip(\n",
    "        probability=1.0\n",
    "    )  # 100% probability for demo purposes\n",
    "    images = fn.flip(images, horizontal=should_flip)\n",
    "    bboxes = fn.bb_flip(bboxes, horizontal=should_flip, ltrb=True)\n",
    "    vertices = fn.coord_flip(vertices, flip_x=should_flip)\n",
    "\n",
    "    return images, bboxes, labels, polygons, vertices\n",
    "\n",
    "\n",
    "coco_pipe_bbox_flip = coco_pipeline_bbox_flip(\n",
    "    batch_size=batch_size, num_threads=n_threads, device_id=0, seed=seed\n",
    ")\n",
    "coco_pipe_bbox_flip.build()\n",
    "outputs = coco_pipe_bbox_flip.run()\n",
    "show(outputs, relative_coords=True, title=\"coco_pipeline_bbox_flip\", index=3, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def\n",
    "def coco_pipeline_bbox_random():\n",
    "    encoded, bboxes, labels, polygons, vertices = coco_reader_def(ratio=True)\n",
    "    images = fn.decoders.image(\n",
    "        encoded, device=\"mixed\", output_type=types.RGB, hw_decoder_load=0.75\n",
    "    )\n",
    "    input_shape = fn.peek_image_shape(encoded, dtype=types.INT32)[:2]\n",
    "    anchor_rel, shape_rel, bboxes, labels, bbox_indices = fn.random_bbox_crop(\n",
    "        bboxes,\n",
    "        labels,\n",
    "        aspect_ratio=[0.5, 2],\n",
    "        thresholds=[0.0],\n",
    "        allow_no_crop=False,\n",
    "        scaling=[0.3, 0.6],\n",
    "        seed=seed,\n",
    "        bbox_layout=\"xyXY\",\n",
    "        output_bbox_indices=True,\n",
    "    )\n",
    "\n",
    "    # Partial decoding of the image\n",
    "    images = fn.decoders.image_slice(\n",
    "        encoded,\n",
    "        anchor_rel,\n",
    "        shape_rel,\n",
    "        normalized_anchor=True,\n",
    "        normalized_shape=True,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    # Cropped image dimensions\n",
    "    crop_shape = fn.shapes(images, dtype=types.FLOAT)\n",
    "    crop_h = fn.slice(crop_shape, 0, 1, axes=[0])\n",
    "    crop_w = fn.slice(crop_shape, 1, 1, axes=[0])\n",
    "\n",
    "    images = images.gpu()\n",
    "\n",
    "    # Adjust masks coordinates to the coordinate space of the cropped image, while also converting\n",
    "    # relative to absolute coordinates by mapping the top-left corner (anchor_rel_x, anchor_rel_y), to (0, 0)\n",
    "    # and the bottom-right corner (anchor_rel_x+shape_rel_x, anchor_rel_y+shape_rel_y) to (crop_w, crop_h)\n",
    "    MT_vertices = fn.transforms.crop(\n",
    "        from_start=anchor_rel,\n",
    "        from_end=(anchor_rel + shape_rel),\n",
    "        to_start=(0.0, 0.0),\n",
    "        to_end=fn.cat(crop_w, crop_h),\n",
    "    )\n",
    "    vertices = fn.coord_transform(vertices, MT=MT_vertices)\n",
    "\n",
    "    # Convert bounding boxes to absolute coordinates\n",
    "    MT_bboxes = fn.transforms.crop(\n",
    "        to_start=(0.0, 0.0, 0.0, 0.0), to_end=fn.cat(crop_w, crop_h, crop_w, crop_h)\n",
    "    )\n",
    "    bboxes = fn.coord_transform(bboxes, MT=MT_bboxes)\n",
    "\n",
    "    return images, bboxes, labels, polygons, vertices\n",
    "\n",
    "\n",
    "coco_pipe_bbox_random = coco_pipeline_bbox_random(\n",
    "    batch_size=batch_size, num_threads=n_threads, device_id=0, seed=seed\n",
    ")\n",
    "coco_pipe_bbox_random.build()\n",
    "outputs = coco_pipe_bbox_random.run()\n",
    "show(outputs, title=\"coco_pipeline_bbox_random\", index=3, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a866c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_speedtest:\n",
    "    pipelines_speedtest = {\n",
    "        \"coco_pipeline\": coco_pipeline,\n",
    "        \"coco_pipeline_bbox_flip\": coco_pipeline_bbox_flip,\n",
    "        \"coco_pipeline_bbox_random\": coco_pipeline_bbox_random,\n",
    "    }\n",
    "\n",
    "    for k, v in pipelines_speedtest.items():\n",
    "        print(\n",
    "            \"{}: {}\".format(\n",
    "                k, speedtest(v, speedtest_batch_size, n_threads, device_id=1)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86efc4b",
   "metadata": {},
   "source": [
    "___\n",
    "## Numpy Reader with Volumetric Data\n",
    "\n",
    "This example shows how to read Numpy array files (*.npy), with DALI's ``readers.numpy`` reader. This notebook also shows how to use DALI to load numpy files directly to GPU memory, thanks to NVIDIA GPUDirect Storage, and how to use the region-of-interest (ROI) API to load regions of the array.\n",
    "\n",
    "The reader extracts the shape and the data type information directly from the files. Please note that only Numpy v1 (and not v2) files are currently supported. Numpy v1 are the most commonly used. See the [numpy file format specification](https://numpy.org/neps/nep-0001-npy-format.html) for more details.\n",
    "\n",
    "The operator returns arrays with shapes taken from the files. DALI tensors are always stored in C (row-major) order. If the files contain the data in FORTRAN (column-major) order, the operator will automatically transpose the data to C order. This transposition adds significant time to the loading process. Therefore, we recommend storing files in C order when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49126bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_2d = os.path.join(\n",
    "    dali_extra_dir, \"db\", \"3D\", \"MRI\", \"Knee\", \"npy_2d_slices\", \"STU00001\"\n",
    ")\n",
    "data_dir_3d = os.path.join(\n",
    "    dali_extra_dir, \"db\", \"3D\", \"MRI\", \"Knee\", \"npy_3d\", \"STU00001\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch(np_arrays, nsamples=None, dpi=60):\n",
    "    if nsamples is None:\n",
    "        nsamples = len(np_arrays)\n",
    "    fig, axvec = plt.subplots(\n",
    "        nrows=1, ncols=nsamples, figsize=(10, 10 * nsamples), dpi=dpi\n",
    "    )\n",
    "    for i in range(nsamples):\n",
    "        ax = axvec[i]\n",
    "        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "        ax.imshow(Image.fromarray(np_arrays[i]))\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def run(p):\n",
    "    p.build()  # build the pipeline\n",
    "    outputs = p.run()  # Run once\n",
    "    # Getting the batch as a list of numpy arrays, for displaying\n",
    "    batch = [np.array(outputs[0][s]) for s in range(batch_size)]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "ser00001_2d = [\n",
    "    np.load(os.path.join(data_dir_2d, \"SER00001\", f\"{i}.npy\")) for i in range(4)\n",
    "]\n",
    "plot_batch(ser00001_2d, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ser00001_3d = np.load(os.path.join(data_dir_3d, \"SER00001.npy\"))\n",
    "plot_batch([ser00001_3d[0], ser00001_3d[1], ser00001_3d[2], ser00001_3d[3]], dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf582b",
   "metadata": {},
   "source": [
    "#### Region-of-interest (ROI) API\n",
    "\n",
    "In the example shown above, we see that the relevant data is concentrated in the upper left quadrant of the image and the rest does not contain useful information. This is not true for all the images in the dataset, but it will serve us as a good example to demonstrate ROI reading.\n",
    "\n",
    "Numpy reader allows the user to specify a region of interest, equivalent to the arguments specified to ``slice`` operation. The benefit is that the reader will only read the relevant part of the file, saving I/O bandwidth and memory utilization. Note that setting the ``dont_use_mmap`` argument to False will negate that performance benefit, with the entire file being read first and then sliced.\n",
    "\n",
    "The ROI can be specified in absolute or relative terms, and can be specified on a subset of the array's axes. For dimensions not specified in the ROI, the whole extent of the array shall be used. (see arguments ``roi_start``, ``rel_roi_start``, ``roi_end``, ``rel_roi_end``, ``roi_shape``, ``rel_roi_shape``, ``axes``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def(batch_size=batch_size, num_threads=n_threads, device_id=0)\n",
    "def pipe_roi1():\n",
    "    data = fn.readers.numpy(\n",
    "        device=\"cpu\",\n",
    "        file_root=samp_2d_data_dir,\n",
    "        files=samp_2d_files,\n",
    "        rel_roi_start=[0.1, 0.01],\n",
    "        rel_roi_end=[0.4, 0.5],\n",
    "        read_ahead=True,\n",
    "        dont_use_mmap=True,\n",
    "        pad_last_batch=True,\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "samp_2d_data_dir = os.path.join(data_dir_2d, \"SER00001\")\n",
    "samp_2d_files = [\"0.npy\"]\n",
    "data_roi1 = run(pipe_roi1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d81dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch(data_roi1, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b253a3",
   "metadata": {},
   "source": [
    "#### GPUDirect Storage Support\n",
    "DALI Numpy Reader supports [GPUDirect Storage (GDS)](https://developer.nvidia.com/gpudirect-storage) via libcufile. GDS enables a direct data path between storage and GPU memory and avoids extra copies through a bounce buffer in the CPU's memory.\n",
    "\n",
    "In order to enable GDS support in DALI, make sure GDS is installed. On Systems with CUDA 11.4 or newer, GDS is already installed as part of the CUDA 11.4 SDK. For older CUDA releases, please install GDS separately (follow the link above for instructions).\n",
    "\n",
    "Once GDS is installed, it can be used by simply switching the device of the reader to ``\"gpu\"``. Note that if GDS is not available, you will likely see a CUDA Driver API error when trying to execute the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def(batch_size=batch_size, num_threads=n_threads, device_id=0)\n",
    "def pipe_gds():\n",
    "    data = fn.readers.numpy(\n",
    "        device=\"gpu\",\n",
    "        file_root=samp_2d_data_dir,\n",
    "        files=samp_2d_files,\n",
    "        rel_roi_start=[0.1, 0.01],\n",
    "        rel_roi_end=[0.4, 0.5],\n",
    "        read_ahead=True,\n",
    "        dont_use_mmap=True,\n",
    "        pad_last_batch=True,\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "samp_2d_data_dir = os.path.join(data_dir_2d, \"SER00001\")\n",
    "samp_2d_files = [\"0.npy\"]\n",
    "p = pipe_gds()\n",
    "p.build()\n",
    "pipe_out = p.run()\n",
    "\n",
    "data_gds = pipe_out[0].as_cpu().as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f06561",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch(data_gds, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e752fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def(batch_size=batch_size, num_threads=n_threads, device_id=0)\n",
    "def pipe_3d_gds():\n",
    "    data = fn.readers.numpy(\n",
    "        device=\"gpu\",\n",
    "        file_root=samp_3d_data_dir,\n",
    "        files=samp_3d_files,\n",
    "        #         rel_roi_start=[0, 0.1, 0.01],\n",
    "        #         rel_roi_end=[1, 0.4, 0.5],\n",
    "        read_ahead=True,\n",
    "        dont_use_mmap=True,\n",
    "        pad_last_batch=True,\n",
    "    )\n",
    "\n",
    "    data = fn.transpose(data, perm=[1, 2, 0])\n",
    "    data = fn.resize(\n",
    "        data,\n",
    "        resize_x=320,\n",
    "        resize_y=320,\n",
    "        mode=\"stretch\",\n",
    "        interp_type=types.INTERP_LANCZOS3,\n",
    "        mag_filter=types.INTERP_LANCZOS3,\n",
    "    )\n",
    "    data = fn.transpose(data, perm=[2, 0, 1])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Tensors in the list must have the same shape\n",
    "samp_3d_data_dir = data_dir_3d\n",
    "samp_3d_files = [\"SER00005.npy\"]\n",
    "\n",
    "p = pipe_3d_gds()\n",
    "p.build()\n",
    "pipe_out = p.run()\n",
    "\n",
    "data_gds = pipe_out[0].as_cpu().as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca05b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch(data_gds[0], dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f459bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_speedtest:\n",
    "    pipelines_speedtest = {\n",
    "        \"pipe_roi1\": pipe_roi1,\n",
    "        \"pipe_gds\": pipe_gds,\n",
    "    }\n",
    "\n",
    "    for k, v in pipelines_speedtest.items():\n",
    "        print(\n",
    "            \"{}: {}\".format(\n",
    "                k, speedtest(v, speedtest_batch_size, n_threads, device_id=1)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a0c4a",
   "metadata": {},
   "source": [
    "___\n",
    "## Audio Spectrogram\n",
    "\n",
    "In this example we will go through the steps to build a DALI audio processing pipeline, including the calculation of a spectrogram. A spectrogram is a representation of a signal (e.g. an audio signal) that shows the evolution of the frequency spectrum in time.\n",
    "\n",
    "Typically, a spectrogram is calculated by computing the fast fourier transform (FFT) over a series of overlapping windows extracted from the original signal. The process of dividing the signal in short term sequences of fixed size and applying FFT on those independently is called Short-time Fourier transform (STFT). The spectrogram is then calculated as the (typically squared) complex magnitude of the STFT.\n",
    "\n",
    "Extracting short term windows of the original image affects the calculated spectrum by producing aliasing artifacts. This is often called spectral leakage. To control/reduce the spectral leakage effect, we use different window functions when extracting the windows. Some examples of window functions are: Hann, Hanning, etc.\n",
    "\n",
    "It is beyond the scope of this example to go deeper into the details of the signal processing concepts we mentioned above. More information can be found here:\n",
    "- [STFT](https://en.wikipedia.org/wiki/Short-time_Fourier_transform)\n",
    "- [Window functions](https://en.wikipedia.org/wiki/Window_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_spectrogram(spec, title, sr, hop_length, y_axis=\"log\", x_axis=\"time\", dpi=60):\n",
    "    fig, ax = plt.subplots(dpi=dpi)\n",
    "    librosa.display.specshow(\n",
    "        spec, sr=sr, y_axis=y_axis, x_axis=x_axis, hop_length=hop_length\n",
    "    )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.colorbar(format=\"%+2.0f dB\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = os.path.join(dali_extra_dir, \"db\", \"audio\", \"wav\", \"237-134500-0000.wav\")\n",
    "\n",
    "# Size of the FFT, which will also be used as the window length\n",
    "n_fft = 2048\n",
    "\n",
    "# Step or stride between windows. If the step is smaller than the window lenght, the windows will overlap\n",
    "hop_length = 512\n",
    "\n",
    "# Load sample audio file\n",
    "y, sr = librosa.load(sample_data)\n",
    "\n",
    "# Calculate the spectrogram as the square of the complex magnitude of the STFT\n",
    "spectrogram_librosa = (\n",
    "    np.abs(\n",
    "        librosa.stft(\n",
    "            y,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            win_length=n_fft,\n",
    "            window=\"hann\",\n",
    "            pad_mode=\"reflect\",\n",
    "        )\n",
    "    )\n",
    "    ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4588f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_librosa_db = librosa.power_to_db(spectrogram_librosa, ref=np.max)\n",
    "show_spectrogram(\n",
    "    spectrogram_librosa_db, \"Librosa power spectrogram\", sr, hop_length, dpi=dpi\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeec93c",
   "metadata": {},
   "source": [
    "### Calculating the Spectrogram using DALI\n",
    "\n",
    "To demonstrate DALI's [spectrogram](../../operations/nvidia.dali.fn.spectrogram.html) operator we will define a DALI pipeline. For demonstration purposes, we can just feed the same input in every iteration, as we will be only calculating one spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d815e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def\n",
    "def spectrogram_pipe(nfft, window_length, window_step, device=\"cpu\"):\n",
    "    audio = types.Constant(device=device, value=audio_data)\n",
    "    spectrogram = fn.spectrogram(\n",
    "        audio,\n",
    "        device=device,\n",
    "        nfft=nfft,\n",
    "        window_length=window_length,\n",
    "        window_step=window_step,\n",
    "    )\n",
    "    return spectrogram\n",
    "\n",
    "\n",
    "audio_data = np.array(y, dtype=np.float32)\n",
    "\n",
    "pipe = spectrogram_pipe(\n",
    "    device=\"gpu\",\n",
    "    batch_size=1,\n",
    "    num_threads=n_threads,\n",
    "    device_id=0,\n",
    "    nfft=n_fft,\n",
    "    window_length=n_fft,\n",
    "    window_step=hop_length,\n",
    ")\n",
    "pipe.build()\n",
    "outputs = pipe.run()\n",
    "spectrogram_dali = outputs[0][0].as_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_dali_db = librosa.power_to_db(spectrogram_dali, ref=np.max)\n",
    "show_spectrogram(spectrogram_dali_db, \"DALI power spectrogram\", sr, hop_length, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28958ea",
   "metadata": {},
   "source": [
    "___\n",
    "## Geometric Transforms\n",
    "\n",
    "In this example we demonstrate the operators from `transforms` module and how they can be used for transforming images and point clouds.\n",
    "\n",
    "\n",
    "\n",
    "### Warp Operators\n",
    "All warp operators work by calculating the output pixels by sampling the source image at transformed coordinates:\n",
    "\n",
    "${Out}(x, y) = {In}(x_{src}, y_{src})$\n",
    "\n",
    "This way each output pixel is calculated exactly once.\n",
    "\n",
    "If the source coordinates do not point exactly to pixel centers, the values of neighboring pixels will be interpolated or the nearst pixel is taken, depending on the interpolation method specified in the `interp_type` argument.\n",
    "\n",
    "### Affine Transform\n",
    "\n",
    "The operators from `transforms` module can generate and combine transform matrices for different kinds of affine transforms. An affine transform is defined by the formula:\n",
    "\n",
    "$\n",
    "X_{out}\n",
    "= \\begin{vmatrix}\n",
    "M & T\n",
    "\\end{vmatrix}\n",
    "\\begin{vmatrix}\n",
    "X_{in} \\\\\n",
    "1\n",
    "\\end{vmatrix}\n",
    "$\n",
    "\n",
    "Where $X_{in}$ is an input point, $X_{out}$ - the corresponding output, $M$ - linear part of the transformation and $T$ - a translation vector.\n",
    "\n",
    "If the points are in 2D space, the formula can be written as:\n",
    "\n",
    "$\n",
    "\\begin{vmatrix}\n",
    "x_{out} \\\\\n",
    "y_{out}\n",
    "\\end{vmatrix}\n",
    "= \\begin{vmatrix}\n",
    "m_{00} & m_{01} & t_x \\\\\n",
    "m_{10} & m_{11} & t_y\n",
    "\\end{vmatrix}\n",
    "\\begin{vmatrix}\n",
    "x_{in} \\\\\n",
    "y_{in} \\\\\n",
    "1\n",
    "\\end{vmatrix}\n",
    "$\n",
    "\n",
    "### Transform Catalogue\n",
    "\n",
    "There are several transforms available in `transforms` module. Each of these operators can generate an affine transform matrix and combine it with a pre-existing transform. Here's the list of available transforms:\n",
    "\n",
    "* `rotation` - rotate by given angle (in degrees) around given point and axis (for 3D only)\n",
    "* `translation` - translate by given offset\n",
    "* `scale` - scale by given factor\n",
    "* `shear` - shear by given factors or angles; there are 2 shear factors for 2D and 6 factors for 3D\n",
    "* `crop` - translates and scales so that input corners (`from_start`, `from_end`) map to output corners (`to_start`, `to_end`).\n",
    "\n",
    "The documentation of the operators contains the detailed information about their parameters.\n",
    "\n",
    "There's also the operator `combine` which combines multiple affine transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772621ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_images_sizes(jpegs):\n",
    "    shapes = fn.peek_image_shape(jpegs)  # the shapes are HWC\n",
    "    h, w = shapes[0], shapes[1]  # extract H and W ...\n",
    "    return fn.stack(w, h)  # ...and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a178763",
   "metadata": {},
   "outputs": [],
   "source": [
    "dali_extra_dir = os.environ[\"DALI_EXTRA_PATH\"]\n",
    "root_dir = os.path.join(dali_extra_dir, \"db\", \"face_landmark\")\n",
    "image_files = [\"{}.jpeg\".format(i) for i in range(6)]\n",
    "keypoint_files = [\"{}.npy\".format(i) for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c0294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(images, landmarks, title=\"\", dpi=60):\n",
    "    if hasattr(images, \"as_cpu\"):\n",
    "        images = images.as_cpu()\n",
    "    batch_size = len(images)\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 14), dpi=dpi)\n",
    "    plt.suptitle(None)\n",
    "    columns = 3\n",
    "    rows = int(batch_size / columns)\n",
    "    gs = gridspec.GridSpec(rows, columns)\n",
    "    for i in range(batch_size):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title)\n",
    "        img = images.at(i)\n",
    "        r = 0.002 * max(img.shape[0], img.shape[1])\n",
    "        lm = 0\n",
    "        for p in landmarks.at(i):\n",
    "            circle = patches.Circle(p, r, color=(0, 1, 0, 1))\n",
    "            ax.add_patch(circle)\n",
    "            # uncomment to see show keypoint to feature visually.\n",
    "            # plt.text(p[0], p[1], lm, fontsize=6)\n",
    "            lm += 1\n",
    "        plt.imshow(img)\n",
    "\n",
    "\n",
    "def gallery(pipe_out, titles, dpi=60):\n",
    "    pipe_out = [x.as_cpu() if hasattr(x, \"as_cpu\") else x for x in pipe_out]\n",
    "\n",
    "    batch_size = len(pipe_out[0])\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 24), dpi=dpi)\n",
    "    plt.suptitle(None)\n",
    "    columns = batch_size\n",
    "    rows = len(pipe_out) // 2\n",
    "    gs = gridspec.GridSpec(rows, columns)\n",
    "    flat = 0\n",
    "    for j in range(0, len(pipe_out), 2):\n",
    "        for i in range(batch_size):\n",
    "            ax = plt.subplot(gs[flat])\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"\")\n",
    "            img = pipe_out[j].at(i)\n",
    "            r = 0.002 * max(img.shape[0], img.shape[1])\n",
    "            for p in pipe_out[j + 1].at(i):\n",
    "                circle = patches.Circle(p, r, color=(0, 1, 0, 1))\n",
    "                ax.add_patch(circle)\n",
    "            plt.imshow(img)\n",
    "            if i == 0:\n",
    "                plt.title(titles[j // 2])\n",
    "            flat += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e43ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def\n",
    "def transforms_gallery_pipe():\n",
    "    jpegs, _ = fn.readers.file(file_root=root_dir, files=image_files)\n",
    "    images = fn.decoders.image(jpegs, device=\"mixed\", hw_decoder_load=0.75)\n",
    "    keypoints = fn.readers.numpy(file_root=root_dir, files=keypoint_files)\n",
    "\n",
    "    size = encoded_images_sizes(jpegs)\n",
    "    center = size / 2\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    transforms = [\n",
    "        fn.transforms.translation(offset=fn.random.uniform(range=(-100, 100), shape=2)),\n",
    "        fn.transforms.rotation(angle=fn.random.uniform(range=(-45, 45)), center=center),\n",
    "        fn.transforms.scale(\n",
    "            scale=fn.random.uniform(range=(0.5, 2), shape=[2]), center=center\n",
    "        ),\n",
    "        fn.transforms.shear(\n",
    "            shear=fn.random.uniform(range=(-1, 1), shape=[2]), center=center\n",
    "        ),\n",
    "        fn.transforms.crop(\n",
    "            from_start=size * 0.1,\n",
    "            from_end=size * 0.8,\n",
    "            to_start=[0, 0],\n",
    "            to_end=size * 1.0,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for mt in transforms:\n",
    "        out_img = fn.warp_affine(images, matrix=mt, fill_value=0, inverse_map=False)\n",
    "        out_kp = fn.coord_transform(keypoints, MT=mt)\n",
    "        outputs += [out_img, out_kp]\n",
    "    return tuple(outputs)\n",
    "\n",
    "\n",
    "pipe = transforms_gallery_pipe(\n",
    "    batch_size=6, num_threads=n_threads, device_id=0, seed=seed\n",
    ")\n",
    "pipe.build()\n",
    "pipe_out = pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660241f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery(pipe_out, [\"translation\", \"rotation\", \"scale\", \"shear\", \"crop\"], dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ff118",
   "metadata": {},
   "source": [
    "### Facial Landmark Alignment with DALI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_rotation(points):\n",
    "    center_of_eyes = points[27]\n",
    "    left_eye = points[36]\n",
    "    right_eye = points[45]\n",
    "    dY = right_eye[1] - left_eye[1]\n",
    "    dX = right_eye[0] - left_eye[0]\n",
    "    tan = dY / dX\n",
    "\n",
    "    # radians to degrees: 180/pi\n",
    "    rotation = -1 * (math.atan(tan) * 57.29577951308232)\n",
    "    return rotation, center_of_eyes\n",
    "\n",
    "\n",
    "@pipeline_def\n",
    "def facial_landmark_align_pipe():\n",
    "    jpegs, _ = fn.readers.file(file_root=root_dir, files=image_files)\n",
    "    images = fn.decoders.image(\n",
    "        jpegs, device=\"mixed\", output_type=types.RGB, hw_decoder_load=0.75\n",
    "    )\n",
    "    keypoints = fn.readers.numpy(file_root=root_dir, files=keypoint_files)\n",
    "    rotation, center = get_face_rotation(keypoints)\n",
    "    mt = fn.transforms.rotation(angle=rotation, center=center)\n",
    "    images = fn.warp_affine(images, matrix=mt, fill_value=0, inverse_map=False)\n",
    "    keypoints = fn.coord_transform(keypoints, MT=mt)\n",
    "    return images, keypoints, rotation\n",
    "\n",
    "\n",
    "pipe = facial_landmark_align_pipe(\n",
    "    batch_size=6, num_threads=n_threads, device_id=0, seed=seed\n",
    ")\n",
    "pipe.build()\n",
    "images, keypoints, rotation = pipe.run()\n",
    "\n",
    "show(images, keypoints, title=\"Face Alignment with Warp Affine\", dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_speedtest:\n",
    "    pipelines_speedtest = {\n",
    "        \"transforms_gallery_pipe\": transforms_gallery_pipe,\n",
    "        \"facial_landmark_align_pipe\": facial_landmark_align_pipe,\n",
    "    }\n",
    "\n",
    "    for k, v in pipelines_speedtest.items():\n",
    "        print(\n",
    "            \"{}: {}\".format(\n",
    "                k, speedtest(v, speedtest_batch_size, n_threads, device_id=1)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d174aa40",
   "metadata": {},
   "source": [
    "___\n",
    "## Simple Video Pipeline Reading From Multiple Files\n",
    "\n",
    "In this example, we will go through the creation of a pipeline using the readers.video operator. The pipeline will return a batch of frame sequences. These sequences are an arbitrary number of frames (images). The difference being that images are or dimension HWC whereas sequences are of dimension FHWC.\n",
    "\n",
    "For more information on the readers.video parameters, please look at the documentation.\n",
    "\n",
    "We need some video containers to process. We can use [Sintel](https://en.wikipedia.org/wiki/Sintel) trailer, which is an mp4 container containing an h264 video and distributed under the Create Common license. Letâ€™s split it into 10s clips in order to check how `readers.Video` handles multiple video files. This can be done easily with the `ffmpeg` standalone tool.\n",
    "\n",
    "Then we can set the parameters that will be use in the pipeline. The `count` parameter will define how many frames we want in each sequence sample.\n",
    "\n",
    "We can replace `video_directory` with any other directory containing video container files recognized by __FFmpeg__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf08699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video(sequence, dpi=60):\n",
    "    columns = 4\n",
    "    rows = (sequence_length + 1) // (columns)\n",
    "    fig = plt.figure(figsize=(32, (16 // columns) * rows), dpi=dpi)\n",
    "    gs = gridspec.GridSpec(rows, columns)\n",
    "    for j in range(rows * columns):\n",
    "        plt.subplot(gs[j])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(sequence[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0957431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "sequence_length = 8\n",
    "initial_prefetch_size = 16\n",
    "video_directory = os.path.join(\n",
    "    os.environ[\"DALI_EXTRA_PATH\"], \"db\", \"video\", \"sintel\", \"video_files\"\n",
    ")\n",
    "video_files = [video_directory + \"/\" + f for f in os.listdir(video_directory)]\n",
    "n_iter = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b470389",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def\n",
    "def video_pipe():\n",
    "    videos = fn.readers.video(\n",
    "        device=\"gpu\",\n",
    "        filenames=video_files,\n",
    "        sequence_length=sequence_length,\n",
    "        shard_id=0,\n",
    "        num_shards=1,\n",
    "        random_shuffle=True,\n",
    "        initial_fill=initial_prefetch_size,\n",
    "        file_list_include_preceding_frame=True,\n",
    "    )\n",
    "\n",
    "    resized = fn.resize(\n",
    "        videos,\n",
    "        size=[900, 1600],\n",
    "        mode=\"stretch\",\n",
    "        interp_type=types.INTERP_LANCZOS3,\n",
    "        mag_filter=types.INTERP_LANCZOS3,\n",
    "    )\n",
    "\n",
    "    flipped = fn.flip(resized)\n",
    "    motion = fn.optical_flow(videos)\n",
    "\n",
    "    return flipped, motion\n",
    "\n",
    "\n",
    "pipe = video_pipe(batch_size=batch_size, num_threads=n_threads, device_id=0, seed=seed)\n",
    "pipe.build()\n",
    "pipe_out = pipe.run()\n",
    "sequences_out = pipe_out[0].as_cpu().as_array()\n",
    "motion_out = pipe_out[1].as_cpu().as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(sequences_out[0], dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfac7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_speedtest:\n",
    "    pipelines_speedtest = {\n",
    "        \"video_pipe\": video_pipe,\n",
    "    }\n",
    "\n",
    "    for k, v in pipelines_speedtest.items():\n",
    "        print(\n",
    "            \"{}: {}\".format(\n",
    "                k, speedtest(v, speedtest_batch_size, n_threads, device_id=1)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf516d",
   "metadata": {},
   "source": [
    "___\n",
    "## Triton Model Serialization & Example Performance Delta\n",
    "\n",
    "![](https://developer.nvidia.com/sites/default/files/akamai/triton.png)\n",
    "- __Client preprocessing__: Samples are decoded, resized, and normalized in parallel using OpenCV.\n",
    "- __Server preprocessing__: The Python client script sends encoded images to the server, where the whole DALI preprocessing happens.\n",
    "\n",
    "![](https://developer-blogs.nvidia.com/wp-content/uploads/2021/04/throughput-vs-latency-1.png)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<sub>Throughput vs. latency plots for both scenarios with batches of size 1, 4, 8, 32. The more to the left and to the top, the better the result is. The performance results were collected on a DGX A100 machine.</sub>\n",
    "</div>\n",
    "\n",
    "### Sample Model Preprocessing\n",
    "Facial Recognition using [NVIDIA's pre-trained FaceDetect model](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/facenet)\n",
    "\n",
    "### Model Overview\n",
    "The model described in this card detects one or more faces in the given image / video. Compared to the FaceirNet model, this model gives better results on RGB images and smaller faces.\n",
    "\n",
    "### Model Architecture\n",
    "The model is based on NVIDIA DetectNet_v2 detector with ResNet18 as a feature extractor. This architecture, also known as GridBox object detection, uses bounding-box regression on a uniform grid on the input image. Gridbox system divides an input image into a grid which predicts four normalized bounding-box parameters (xc, yc, w, h) and confidence value per output class.\n",
    "\n",
    "The raw normalized bounding-box and confidence detections needs to be post-processed by a clustering algorithm such as DBSCAN or NMS to produce final bounding-box coordinates and category labels.\n",
    "\n",
    "### Input\n",
    "Grayscale Image whose values in RGB channels are the same. 736 X 416 X 3 Channel Ordering of the Input: NCHW, where N = Batch Size, C = number of channels (3), H = Height of images (416), W = Width of the images (736) Input scale: 1/255.0 Mean subtraction: None\n",
    "\n",
    "### Triton Ensemble Model Example\n",
    "\n",
    "```sh\n",
    "model_repository\n",
    "    â”œâ”€â”€ facedetect\n",
    "    â”‚   â”œâ”€â”€ 1\n",
    "    â”‚   â”‚   â”œâ”€â”€ model.trt\n",
    "    â”‚   â””â”€â”€ config.pbtxt\n",
    "    â”œâ”€â”€ facedetect_ensemble\n",
    "    â”‚   â”œâ”€â”€ 1\n",
    "    â”‚   â””â”€â”€ config.pbtxt\n",
    "    â”œâ”€â”€ facedetect_postprocess\n",
    "    â”‚   â”œâ”€â”€ 1\n",
    "    â”‚   â”‚   â”œâ”€â”€ model.py\n",
    "    â”‚   â”‚   â””â”€â”€ postprocessing\n",
    "    â”‚   â”‚       â”œâ”€â”€ clustering_config_facedetect.prototxt\n",
    "    â”‚   â”‚       â”œâ”€â”€ facenet_postprocessor.py\n",
    "    â”‚   â”‚       â”œâ”€â”€ kitti.py\n",
    "    â”‚   â”‚       â”œâ”€â”€ postprocessor_config_pb2.py\n",
    "    â”‚   â”‚       â”œâ”€â”€ postprocessor_config.proto\n",
    "    â”‚   â”‚       â”œâ”€â”€ postprocessor.py\n",
    "    â”‚   â”‚       â”œâ”€â”€ preprocess_input.py\n",
    "    â”‚   â”‚       â”œâ”€â”€ types\n",
    "    â”‚   â”‚       â”‚   â”œâ”€â”€ annotation.py\n",
    "    â”‚   â”‚       â”‚   â”œâ”€â”€ frame.py\n",
    "    â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py\n",
    "    â”‚   â”‚       â”‚   â””â”€â”€ user_data.py\n",
    "    â”‚   â”‚       â””â”€â”€ utils.py\n",
    "    â”‚   â””â”€â”€ config.pbtxt\n",
    "    â”œâ”€â”€ facedetect_preprocess\n",
    "    â”‚   â”œâ”€â”€ 1\n",
    "    â”‚   â”‚   â”œâ”€â”€ model.dali\n",
    "    â”‚   â””â”€â”€ config.pbtxt\n",
    "```\n",
    "\n",
    "The example below will be able to handle client submissions using the acceleration provided by DALI as a preprocessing step to ensure the inputs to the FaceDetect model are appropriate every time and handled as efficiently as possible. \n",
    "\n",
    "\n",
    "\n",
    "Example `config.pbtxt` for the Dali Preprocessing model:\n",
    "\n",
    "```sh\n",
    "name: \"facenet_preprocess\"\n",
    "backend: \"dali\"\n",
    "default_model_filename: \"model.dali\"\n",
    "max_batch_size: 32\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 1, 16, 32 ]\n",
    "  max_queue_delay_microseconds: 500\n",
    "}\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "  }\n",
    "]\n",
    "  \n",
    "input [\n",
    "    {\n",
    "      name: \"input_image_data\"\n",
    "      data_type: TYPE_UINT8\n",
    "      dims: [ -1 ]\n",
    "    }\n",
    "]\n",
    " \n",
    "output [\n",
    "    {\n",
    "      name: \"input_1\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [ 3, 416, 736 ]\n",
    "    },\n",
    "    {\n",
    "      name: \"true_image_size\"\n",
    "      data_type: TYPE_INT64\n",
    "      dims: [ 3 ]\n",
    "    }\n",
    "]\n",
    "\n",
    "parameters {\n",
    "    key: \"model_description\"\n",
    "    value: {\n",
    "        string_value: \"Reshapes a full size image. Grayscale Image whose values in RGB channels are the same. 736 X 416 X 3 Channel Ordering of the Input: NCHW, where N = Batch Size, C = number of channels (3), H = Height of images (416), W = Width of the images (736) Input scale: 1/255.0 Mean subtraction: None\"\n",
    "    }\n",
    "}\n",
    "parameters {        \n",
    "    key: \"license\"\n",
    "    value: {\n",
    "        string_value: \"Apache 2.0 license: https://www.apache.org/licenses/LICENSE-2.0\"\n",
    "    }\n",
    "}\n",
    "\n",
    "parameters: [\n",
    "  {\n",
    "    key: \"num_threads\"\n",
    "    value: { string_value: \"8\" }\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9746f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_AS = \"/workspace/triton_models/facenet_preprocess/1/model.dali\"\n",
    "\n",
    "\n",
    "class FacenetPipeline:\n",
    "    \"\"\"Grayscale Image whose values in RGB channels are the same. 736 X 416 X 3\n",
    "    Channel Ordering of the Input: NCHW, where N = Batch Size, C = number of\n",
    "    channels (3), H = Height of images (416), W = Width of the images (736)\n",
    "    Input scale: 1/255.0 Mean subtraction: None\n",
    "    https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/facenet\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.raw_image_tensor = fn.external_source(name=\"input_image_data\")\n",
    "        self.shapes = fn.peek_image_shape(self.raw_image_tensor)\n",
    "        self.one_over_255 = 1 / 255.0\n",
    "\n",
    "    def load_images(self):\n",
    "        self.image_tensor = fn.decoders.image(\n",
    "            self.raw_image_tensor, output_type=types.GRAY, device=\"mixed\"\n",
    "        )\n",
    "\n",
    "    def color_space_conversion(self):\n",
    "        self.image_tensor = fn.color_space_conversion(\n",
    "            self.image_tensor, image_type=types.GRAY, output_type=types.RGB\n",
    "        )\n",
    "\n",
    "    def resize_images(self):\n",
    "        self.image_tensor = fn.resize(\n",
    "            self.image_tensor,\n",
    "            resize_x=736,\n",
    "            resize_y=416,\n",
    "            interp_type=types.DALIInterpType.INTERP_LANCZOS3,\n",
    "        )\n",
    "\n",
    "    def transpose_images(self):\n",
    "        self.image_tensor = fn.transpose(self.image_tensor, perm=[2, 0, 1])\n",
    "\n",
    "    @pipeline_def(batch_size=32, num_threads=8)\n",
    "    def facenet_reshape(self):\n",
    "        self.load_images()\n",
    "        self.color_space_conversion()\n",
    "        self.resize_images()\n",
    "        self.transpose_images()\n",
    "\n",
    "        return self.image_tensor * self.one_over_255, self.shapes\n",
    "\n",
    "\n",
    "facenet_pipeline = FacenetPipeline()\n",
    "# NOT RUN\n",
    "# _ = facenet_pipeline.facenet_reshape().serialize(filename=SAVE_AS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4ca198",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
